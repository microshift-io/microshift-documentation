<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.89.4" />

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<link rel="canonical" type="text/html" href="https://microshift.io/docs/">
<link rel="apple-touch-icon" sizes="180x180" href="/images/logo/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/logo/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/logo/favicon-16x16.png">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="mask-icon" href="images/logo/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#00aba9">
<meta name="theme-color" content="#ffffff">

<title>Documentation | MicroShift</title><meta property="og:title" content="Documentation" />
<meta property="og:description" content="MicroShift is a research project that is exploring how OpenShift and Kubernetes can be optimized for small form factor and edge computing." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://microshift.io/docs/" /><meta property="og:site_name" content="MicroShift" />

<meta itemprop="name" content="Documentation">
<meta itemprop="description" content="MicroShift is a research project that is exploring how OpenShift and Kubernetes can be optimized for small form factor and edge computing."><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Documentation"/>
<meta name="twitter:description" content="MicroShift is a research project that is exploring how OpenShift and Kubernetes can be optimized for small form factor and edge computing."/>





<link rel="preload" href="/scss/main.min.74383d8b2c66513431ce06d78f9cea3f5745dd97194594c5acd57ec8cc1c401c.css" as="style">
<link href="/scss/main.min.74383d8b2c66513431ce06d78f9cea3f5745dd97194594c5acd57ec8cc1c401c.css" rel="stylesheet" integrity="">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<meta name="theme-color" content="#326ce5">






<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:url" content="https://microshift.io/docs/">
<meta property="og:title" content="Documentation">
<meta name="twitter:title" content="Documentation">
<meta name="twitter:image" content="/images/favicon.png" />

<meta name="twitter:image:alt" content="MicroShift">

<meta property="og:image" content="/images/favicon.png">

<meta property="og:type" content="article">



<script src="/js/script.js"></script>




<link rel="stylesheet" href="/css/jquery-ui.min.137b1c829b79d35bbaea89c94a60a56165e443ca5dd258afed9f60317c44cc98.css">


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">

		<ul class="navbar-nav mt-2 mt-lg-0">
			
			<li class="nav-item mr-2 mb-lg-0">
				<a class="nav-link active" href="https://github.com/microshift-io/microshift" >Getting Started</span></a>
			</li>
			<li class="nav-item mr-2 mb-lg-0">
				<a class="nav-link active" href="https://github.com/microshift-io/microshift/blob/main/README.md" >Documentation</span></a>
			</li>
			
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/docs/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">Documentation</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-e735cee7e913aa88bc0aa10594d12966">MicroShift Documentation</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-17b09a9dc5200475d8a306f18e63a0f9">Getting Started</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-44d1f42a694db2fe4498e44e76cb79ca">User Documentation</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-cd239422cb5c418ee995f07d941d6bd6">Disconnected deployment</a></li>


    
  
    
    
	
<li>3.2: <a href="#pg-0d2b3789b669c1012e6e8f251fcef6f0">Configuring MicroShift</a></li>


    
  
    
    
	
<li>3.3: <a href="#pg-107e4cf533a2ff324bdac20c14bdf742">Auto-applying Manifests</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-49c842618222bb9ce6ca9ca10964aa29">Networking</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.4.1: <a href="#pg-b0e7cd97514654edfa7e1b5bff515294">Overview</a></li>


    
  
    
    
	
<li>3.4.2: <a href="#pg-93d339436d3fd951d49aab2c806ac37e">Exposing Services</a></li>


    
  
    
    
	
<li>3.4.3: <a href="#pg-a813f4a2f155401b0415650bb22eb277">Firewall</a></li>


    
  
    
    
	
<li>3.4.4: <a href="#pg-0477370f872d1351ebe6eea051d22d47">mDNS</a></li>


    
  
    
    
	
<li>3.4.5: <a href="#pg-1e1613f966091eff100b2129a8472a68">CNI Plugin</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3.5: <a href="#pg-53680479178b1a1c2ff6d5d8d000797c">HowTos</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.5.1: <a href="#pg-c49e8fdeb5e48b453ab9c886672f7837">MicroShift with Advanced Cluster Management</a></li>


    
  
    
    
	
<li>3.5.2: <a href="#pg-067fcdf825875018d80f0acbac8fe3f0">Deploying MicroShift behind Proxy</a></li>


    
  
    
    
	
<li>3.5.3: <a href="#pg-bc46ef369ec8c5ad7e090b2e0a105779">Private registries and pull secrets</a></li>


    
  
    
    
	
<li>3.5.4: <a href="#pg-377f4900168fa9986825e624d7f6f4c4">Deploy a basic application</a></li>


    
  
    
    
	
<li>3.5.5: <a href="#pg-5fbbf9155094d477254e7a9708a94dfd">Dynamic Provisioning of PVs</a></li>


    
  
    
    
	
<li>3.5.6: <a href="#pg-7c8cb1c1a13c6df4a3f62a4c5665b735">Offline/disconnected container images</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3.6: <a href="#pg-da827023455bf1040faa66fb1ec83e44">Troubleshooting</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-1ec80b85799e8d84209d74c8205080a1">Developer Documentation</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-ee7318597e2a727a11960bedd6dffd3f">Building and Running MicroShift</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-2ecd4c5c186af77b7fb7444586806f93"></a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-8ee56599b9f5a3e95a8e3e124395fe92">Optional vagrant setup</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-c09f1065b07f2ae3bb609740a3f10dd0">Community</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-c9da5865a77748e4db7d4db31200b34f">Release notes</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-cb28f35491c0139ab4f6852cc850169f">Differences between MicroShift and OKD</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-e735cee7e913aa88bc0aa10594d12966">1 - MicroShift Documentation</h1>
    <div class="lead">MicroShift is a project that is exploring how OpenShift and Kubernetes can be optimized for small form factor and edge computing.</div>
	
</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-17b09a9dc5200475d8a306f18e63a0f9">2 - Getting Started</h1>
    <div class="lead">MicroShift system requirements and deployment</div>
	<h2 id="system-requirements">System Requirements</h2>
<p>To run MicroShift, you need a machine with at least:</p>
<ul>
<li>a supported 64-bit CPU architecture (amd64/x86_64, arm64, or riscv64)</li>
<li>a supported OS (see below)</li>
<li>2 CPU cores</li>
<li>2GB of RAM</li>
<li>1GB of free storage space for MicroShift</li>
</ul>
<h2 id="deploying-microshift-on-edge-devices">Deploying MicroShift on Edge Devices</h2>
<div class="alert alert-danger warning callout" role="alert">
  <strong></strong> The available community documentation is not currently compatible with the latest MicroShift source code.
It is recommended to follow the instructions in the <a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12">MicroShift developer preview documentation</a>, using one of the <a href="https://developers.redhat.com/blog/2021/02/10/how-to-activate-your-no-cost-red-hat-enterprise-linux-subscription">no-cost Red Hat Developer subscriptions</a>.
</div>

<p>We recommend (and only test) deploying MicroShift on RHEL 8, CentOS Stream, or Fedora 34+ installing via RPM (e.g. for embedding MicroShift into an <a href="https://microshift.io/docs/getting-started/#microshift-on-ostree-based-systems"><code>rpm-ostree</code></a> image).</p>
<p>This installation techique has a minimal resource footprint, a strong security posture, the ability to restart/update without disrupting workloads, and optionally auto-updates.</p>
<h3 id="install-cri-o">Install CRI-O</h3>
<p>MicroShift requires CRI-O to be installed and running on the host:</p>
<ul class="nav nav-tabs" id="tabset-docs-getting-started-1" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabset-docs-getting-started-1-0" role="tab" aria-controls="tabset-docs-getting-started-1-0" aria-selected="true">RHEL</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabset-docs-getting-started-1-1" role="tab" aria-controls="tabset-docs-getting-started-1-1">Fedora</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabset-docs-getting-started-1-2" role="tab" aria-controls="tabset-docs-getting-started-1-2">CentOS Stream</a></li></ul>
<div class="tab-content" id="tabset-docs-getting-started-1"><div id="tabset-docs-getting-started-1-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabset-docs-getting-started-1-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash"><span style="color:#a2f">command</span> -v subscription-manager &amp;&gt; /dev/null <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    <span style="color:#666">&amp;&amp;</span> subscription-manager repos --enable rhocp-4.8-for-rhel-8-x86_64-rpms
sudo dnf install -y cri-o cri-tools
sudo systemctl <span style="color:#a2f">enable</span> crio --now
</code></pre></div></div>
  <div id="tabset-docs-getting-started-1-1" class="tab-pane" role="tabpanel" aria-labelledby="tabset-docs-getting-started-1-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf module <span style="color:#a2f">enable</span> -y cri-o:1.21
sudo dnf install -y cri-o cri-tools
sudo systemctl <span style="color:#a2f">enable</span> crio --now
</code></pre></div></div>
  <div id="tabset-docs-getting-started-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-docs-getting-started-1-2">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf module <span style="color:#a2f">enable</span> -y cri-o:1.21
sudo dnf install -y cri-o cri-tools
sudo systemctl <span style="color:#a2f">enable</span> crio --now
</code></pre></div></div></div>

<br/>
<h3 id="deploying-microshift">Deploying MicroShift</h3>
<p>The following steps will deploy MicroShift and enable <code>firewalld</code>. It is always best practice to have firewalls enabled and only to allow the minimum set of ports necessary for MicroShift to operate. <code>Iptables</code> can be used in place of <code>firewalld</code> if desired.</p>
<ul class="nav nav-tabs" id="tabset-docs-getting-started-2" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabset-docs-getting-started-2-0" role="tab" aria-controls="tabset-docs-getting-started-2-0" aria-selected="true">.rpm</a></li>
	  </ul>
<div class="tab-content" id="tabset-docs-getting-started-2"><div id="tabset-docs-getting-started-2-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabset-docs-getting-started-2-0">

<p><p>To have <code>systemd</code> start and manage MicroShift on an rpm-based host, run:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf copr <span style="color:#a2f">enable</span> -y @redhat-et/microshift
sudo dnf install -y microshift
sudo firewall-cmd --zone<span style="color:#666">=</span>trusted --add-source<span style="color:#666">=</span>10.42.0.0/16 --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>80/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>443/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>5353/udp --permanent
sudo firewall-cmd --reload
sudo systemctl <span style="color:#a2f">enable</span> microshift --now
</code></pre></div></div></div>

<p>For more details on MicroShift ports and firewall settings, please see the
<a href="../user-documentation/networking/firewall.md">firewall documentation</a>.</p>
<h3 id="install-clients">Install Clients</h3>
<p>To access the cluster, install the OpenShift client or kubectl.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">curl -O https://mirror.openshift.com/pub/openshift-v4/<span style="color:#a2f;font-weight:bold">$(</span>uname -m<span style="color:#a2f;font-weight:bold">)</span>/clients/ocp/stable/openshift-client-linux.tar.gz
sudo tar -xf openshift-client-linux.tar.gz -C /usr/local/bin oc kubectl
</code></pre></div><h3 id="copy-kubeconfig">Copy Kubeconfig</h3>
<p>Copy the kubeconfig to the default location that can be accessed without administrator privilege.</p>
<ul class="nav nav-tabs" id="tabset-docs-getting-started-3" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabset-docs-getting-started-3-0" role="tab" aria-controls="tabset-docs-getting-started-3-0" aria-selected="true">.rpm</a></li>
	  </ul>
<div class="tab-content" id="tabset-docs-getting-started-3"><div id="tabset-docs-getting-started-3-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabset-docs-getting-started-3-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">mkdir ~/.kube
sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config
</code></pre></div></div></div>

<p>It is now possible to run kubectl or oc commands against the MicroShift environment.
Verify that MicroShift is running:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc get pods -A
</code></pre></div><h2 id="microshift-on-ostree-based-systems">MicroShift on OSTree based systems</h2>
<p>As mentioned aboved, MicroShift has been designed to be deployed on edge computing devices. Looking at security standards,
an edge optimized operating system will most likely be inmutable and based in transactions for upgrades and rollbacks. OSTree provides these capabilities.</p>
<p>Fedora IoT and RHEL for Edge are both OSTree based systems and MicroShift can be shipped as part of the base <code>rpm-ostree</code>.
The recommended way to embed MicroShift in these operating systems is to build your own <code>rpm-ostree</code> with tools like <a href="https://fedoramagazine.org/introduction-to-image-builder/">Image Builder</a>. This project will allow you to create your own customized version of Fedora IoT or RHEL for Edge in order to include MicroShift and all the required dependencies like CRI-O.</p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-44d1f42a694db2fe4498e44e76cb79ca">3 - User Documentation</h1>
    <div class="lead">After MicroShift is up and running, what's next? Follow the user documentation to explore. Here you'll find a section of <code>HowTos</code> to get you going.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-cd239422cb5c418ee995f07d941d6bd6">3.1 - Disconnected deployment</h1>
    <div class="lead">MicroShift can run without internet connectivity.</div>
	<h2 id="wip-content-coming-soon">WIP Content coming soon</h2>
<p>Pre-load MicroShift image tarball into CRI-O</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0d2b3789b669c1012e6e8f251fcef6f0">3.2 - Configuring MicroShift</h1>
    <div class="lead">Configuration options with MicroShift</div>
	<h2 id="configuration">Configuration</h2>
<p>Microshift can be configured in three simple ways, in order of precedence:</p>
<ul>
<li>Commandline arguments</li>
<li>Environment variables</li>
<li>Configuration file</li>
</ul>
<p>An example configuration can be found <a href="https://github.com/openshift/microshift/blob/main/test/config.yaml">here</a>.</p>
<p>Below is a table of consisting of the configuration settings presently offered in MicroShift, along with the ways they can be set, what they mean, and their default values.</p>
<table>
<thead>
<tr>
<th>MicroshiftConfig field</th>
<th>CLI Argument</th>
<th>Environment Variable</th>
<th>Configuration File</th>
<th>Meaning</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataDir</code></td>
<td><code>--data-dir</code></td>
<td><code>MICROSHIFT_DATADIR</code></td>
<td><code>.dataDir</code></td>
<td>Data directory for MicroShift</td>
<td><code>&quot;~/.microshift/data&quot;</code></td>
</tr>
<tr>
<td><code>LogDir</code></td>
<td><code>--log-dir</code></td>
<td><code>MICROSHIFT_LOGDIR</code></td>
<td><code>.logDir</code></td>
<td>Directory to output logfiles to</td>
<td><code>&quot;&quot;</code></td>
</tr>
<tr>
<td><code>LogVLevel</code></td>
<td><code>--v</code></td>
<td><code>MICROSHIFT_LOGVLEVEL</code></td>
<td><code>.logVLevel</code></td>
<td>Log verbosity level</td>
<td><code>0</code></td>
</tr>
<tr>
<td><code>LogVModule</code></td>
<td><code>--vmodule</code></td>
<td><code>MICROSHIFT_LOGVMODULE</code></td>
<td><code>.logVModule</code></td>
<td>Log verbosity module</td>
<td><code>&quot;&quot;</code></td>
</tr>
<tr>
<td><code>LogAlsotostderr</code></td>
<td><code>--alsologtostderr</code></td>
<td><code>MICROSHIFT_LOGALSOTOSTDERR</code></td>
<td><code>.logAlsotostderr</code></td>
<td>Log into standard error as well</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>Roles</code></td>
<td><code>--roles</code></td>
<td><code>MICROSHIFT_ROLES</code></td>
<td><code>.roles</code></td>
<td>Roles available on the cluster</td>
<td><code>[&quot;controlplane&quot;, &quot;node&quot;]</code></td>
</tr>
<tr>
<td><code>NodeName</code></td>
<td><code>n/a</code></td>
<td><code>MICROSHIFT_NODENAME</code></td>
<td><code>.nodeName</code></td>
<td>Name of the node to run MicroShift on</td>
<td><code>os.Hostname()</code></td>
</tr>
<tr>
<td><code>NodeIP</code></td>
<td><code>n/a</code></td>
<td><code>MICROSHIFT_NODEIP</code></td>
<td><code>.nodeIP</code></td>
<td>Node's IP</td>
<td><code>util.GetHostIP()</code></td>
</tr>
<tr>
<td><code>Cluster.URL</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.url</code></td>
<td>URL that the cluster will run on</td>
<td><code>&quot;https://127.0.0.1:6443&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.ClusterCIDR</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.clusterCIDR</code></td>
<td>Cluster's CIDR</td>
<td><code>&quot;10.42.0.0/16&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.ServiceCIDR</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.serviceCIDR</code></td>
<td>Service CIDR</td>
<td><code>&quot;10.43.0.0/16&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.DNS</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.dns</code></td>
<td>Cluster's DNS server</td>
<td><code>&quot;10.43.0.10&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.Domain</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.domain</code></td>
<td>Cluster's domain</td>
<td><code>&quot;cluster.local&quot;</code></td>
</tr>
<tr>
<td><code>ConfigFile</code></td>
<td><code>--config</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td>Path to a config file used to populate the rest of the values</td>
<td><code>&quot;~/.microshift/config.yaml&quot;</code> if the file exists, else <code>/etc/microshift/config.yaml</code> if it exists, else <code>&quot;&quot;</code></td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-107e4cf533a2ff324bdac20c14bdf742">3.3 - Auto-applying Manifests</h1>
    <div class="lead">Automatically applying manifests for bootstrapping cluster services.</div>
	<p>A common use case after bringing up a new cluster is applying manifests for bootstrapping a <a href="https://microshift.io/docs/user-documentation/how-tos/acm-with-microshift/">management agent like the Open Cluster Management's klusterlet</a> or for starting up services when running disconnected.</p>
<p>MicroShift leverages <a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/"><code>kustomize</code></a> for Kubernetes-native templating and declarative management of resource objects. Upon start-up, it searches <code>/etc/microshift/manifests</code>, <code>/usr/lib/microshift/manifests</code> and
<code>${DATADIR}/manifests</code> (which defaults to <code>/var/lib/microshift/manifests</code>) for a <code>kustomization.yaml</code> file. If it finds one, it automatically runs <code>kubectl apply -k</code> to that kustomization`</p>
<p>Example:</p>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt;/etc/microshift/manifests/nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: NGINX_IMAGE
        ports:
        - containerPort: 8080
EOF

cat &lt;&lt;EOF &gt;/etc/microshift/manifests/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - nginx.yaml
images:
  - name: NGINX_IMAGE
    newName: nginx:1.21
EOF
</code></pre><p>The reason for providing multiple directories is to allow a flexible method to manage
MicroShift workloads.</p>
<table>
<thead>
<tr>
<th>Location</th>
<th>Intent</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/etc/microshift/manifests</code></td>
<td>R/W location for configuration management systems or development</td>
</tr>
<tr>
<td><code>/usr/lib/microshift/manifests</code></td>
<td>RO location, for embedding configuration manifests on ostree based systems</td>
</tr>
<tr>
<td><code>${DATADIR}/manifests</code></td>
<td>R/W location for backwards compatibility (deprecated)</td>
</tr>
</tbody>
</table>
<p>The list of manifest locations can be customized via configuration using the manifests section (see
<a href="https://github.com/openshift/microshift/blob/main/test/config.yaml">here</a>) or via the <code>MICROSHIFT_MANIFESTS</code>
environment variable as comma separated directories.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-49c842618222bb9ce6ca9ca10964aa29">3.4 - Networking</h1>
    <div class="lead">Understanding and configuring networking in MicroShift.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-b0e7cd97514654edfa7e1b5bff515294">3.4.1 - Overview</h1>
    <div class="lead">Overview of the MicroShift networking.</div>
	<p>MicroShift uses the host configured networking, either statically configured
or via DHCP. In the case of dynamic addresses MicroShift will restart if an
IP change is detected during runtime.</p>
<p>Connectivity to the K8s API endpoint is provided in the default 6443 port on
the master host(s) IP addresses. If other services in the network must interact
with the MicroShift API connectivity should be performed in any of the following
ways:</p>
<ul>
<li>DNS discovery, pre-configured on the network servers.</li>
<li>Direct IP address connectivity.</li>
<li>mDNS discovery via .local domain, see <a href="../mdns">mDNS</a></li>
</ul>
<p>Conectivity between Pods is handled by the <a href="../cni">CNI</a> plugin on the Pod network
range which defaults to <code>10.42.0.0/16</code> which can be modified via <code>Cluster.ClusterCIDR</code>
<a href="../../configuring/">configuration</a> parameter, see more details in the corresponding sections.</p>
<p>Connectivity to services of type ClusterIP is provided by the embedded <code>kube-proxy</code>
iptables-based implementation on the <code>10.43.0.0/16</code> range which can be modified via
<code>Cluster.ServiceCIDR</code> <a href="../../configuring/">configuration</a> parameter.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-93d339436d3fd951d49aab2c806ac37e">3.4.2 - Exposing Services</h1>
    <div class="lead">Exposing services in MicroShift.</div>
	<p>Services deployed in MicroShift can be exposed in multiple ways.</p>
<h1 id="routes-and-ingresses">Routes and Ingresses</h1>
<p>By default an OpenShift router is created and exposed on host network ports 80/443.
<code>Routes</code>or <code>Ingresses</code> can be used to expose HTTP or HTTPS services through the router.</p>
<h3 id="example">Example</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc create deployment nginx --image<span style="color:#666">=</span>nginxinc/nginx-unprivileged:stable-alpine
oc expose deployment nginx --port<span style="color:#666">=</span><span style="color:#666">8080</span>
oc expose service/nginx --hostname<span style="color:#666">=</span>my-hostname.com

<span style="color:#080;font-style:italic"># assuming my-hostname.com being mapped to the MicroShift node IP</span>
curl http://my-hostname.com
</code></pre></div><h3 id="route-with-mdns-host-example">Route with mDNS host example</h3>
<p>The hostname of a route can be a mDNS (.local) hostname, which would be then
announced via mDNS, see the <a href="../mdns/">mDNS</a> section for more details.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc expose service/nginx --hostname<span style="color:#666">=</span>my-hostname.local
curl http://my-hostname.local
</code></pre></div><h1 id="service-of-type-nodeport">Service of type NodePort</h1>
<p>NodePort type of services expose services over a dedicated port on all the cluster
nodes, such port is routed internally to the active pods backing the service.</p>
<h3 id="example-1">Example</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc create deployment nginx --image<span style="color:#666">=</span>nginxinc/nginx-unprivileged:stable-alpine
oc expose deployment nginx --type<span style="color:#666">=</span>NodePort --name<span style="color:#666">=</span>nodeport-nginx --port <span style="color:#666">8080</span>
<span style="color:#b8860b">NODEPORT</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>oc get service nodeport-nginx -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.ports[0].nodePort}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#b8860b">IP</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>oc get node -A -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[0].status.addresses[0].address}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
curl http://<span style="color:#b8860b">$IP</span>:<span style="color:#b8860b">$NODEPORT</span>/
</code></pre></div><p>For using <code>NodePort</code> services open the 30000-32767 port range , see the
<a href="../firewall/">firewall</a> section.</p>
<h1 id="service-of-type-loadbalancer">Service of type LoadBalancer</h1>
<p>Services of type <code>LoadBalancer</code> are not supported yet, this kind of service is normally backed
by a load balancer in the underlying cloud.</p>
<p>Multiple alternatives are being explored to provide LoadBalancer VIPs on the LAN.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a813f4a2f155401b0415650bb22eb277">3.4.3 - Firewall</h1>
    <div class="lead">Firewall considerations for MicroShift</div>
	<p>MicroShift does not require a firewall to run, but it's recommended. In the case of
using firewalld the following ports should be considered:</p>
<table>
<thead>
<tr>
<th>Port(s)</th>
<th>Protocol(s)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>80</td>
<td><code>TCP</code></td>
<td>HTTP port used to serve applications through the OpenShift router.</td>
</tr>
<tr>
<td>443</td>
<td><code>TCP</code></td>
<td>HTTPS port used to serve applications through the OpenShift router.</td>
</tr>
<tr>
<td>6443</td>
<td><code>TCP</code></td>
<td>HTTPS API port for the MicroShift API</td>
</tr>
<tr>
<td>5353</td>
<td><code>UDP</code></td>
<td><a href="../mdns/">mDNS</a> service to respond for OpenShift route mDNS hosts</td>
</tr>
<tr>
<td>30000-32767</td>
<td><code>TCP/UDP</code></td>
<td>Port range reserved for NodePort type of services, can be used to expose applications on the LAN</td>
</tr>
</tbody>
</table>
<p>Additionally pods need to be able to contact the internal coreDNS server, a way
to allow such connectivity is the following, assuming the PodIP range is <code>10.42.0.0/16</code></p>
<p><code>sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16</code></p>
<h2 id="firewalld">Firewalld</h2>
<p>An example for enabling firewalld and opening all the above mentioned ports is:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf install -y firewalld
sudo systemctl <span style="color:#a2f">enable</span> firewalld --now
sudo firewall-cmd --zone<span style="color:#666">=</span>trusted --add-source<span style="color:#666">=</span>10.42.0.0/16 --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>80/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>443/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>6443/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>5353/udp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>30000-32767/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>30000-32767/udp --permanent
sudo firewall-cmd --reload
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0477370f872d1351ebe6eea051d22d47">3.4.4 - mDNS</h1>
    <div class="lead">embedded Multicast DNS support in MicroShift.</div>
	<p>MicroShift includes an embedded mDNS server for deployment scenarios in which the authoritative DNS server cannot be reconfigured to point clients to services on MicroShift.</p>
<p>mDNS is a protocol used to allow name resolution and service discovery within a LAN using
multicast exposed on the <code>5353/UDP</code> port.</p>
<p>This allows <code>.local</code> domains exposed by MicroShift to be discovered by other elements
on the Local Area Network.</p>
<h2 id="notes-for-linux">Notes for Linux</h2>
<p>mDNS resolution on Linux is provided by the avahi-daemon. For other Linux hosts to discover
MicroShift services or for workers to locate the master node via mDNS, avahi should be enabled:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install -y nss-mdns avahi
hostnamectl set-hostname microshift-vm.local
systemctl <span style="color:#a2f">enable</span> --now avahi-daemon.service
</code></pre></div><p>By default only the minimal IPv4 mDNS resolver is implemented, that will only resolve TLD mDNS
domains like <code>hostname.local</code>, if you want to use hostnames in the form of <code>subdomain.domain.local</code>
you need to enable the full mDNS resolver on the host trying to resolve those dns entries:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> .local &gt; /etc/mdns.allow
<span style="color:#a2f">echo</span> .local. &gt;&gt; /etc/mdns.allow
sed -i <span style="color:#b44">&#39;s/mdns4_minimal/mdns/g&#39;</span> /etc/nsswitch.conf
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1e1613f966091eff100b2129a8472a68">3.4.5 - CNI Plugin</h1>
    <div class="lead">The CNI Plugin used in MicroShift.</div>
	<p>MicroShift uses the <a href="https://github.com/flannel-io/flannel">Flannel</a> CNI network plugin
as lightweight (but less featureful) alternative to OpenShiftSDN or OVNKubernetes.</p>
<p>This provides worker node to worker node pod connectivity via vxlan tunnels.</p>
<p>For single node operation the crio-bridge plugin could be used for additional
resource saving.</p>
<!-- TO-DO: test and documment network-pluging switching -->
<p>Both flannel and crio-bridge have no support for <em>NetworkPolicy</em>.</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53680479178b1a1c2ff6d5d8d000797c">3.5 - HowTos</h1>
    <div class="lead">Solving common use cases with MicroShift.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c49e8fdeb5e48b453ab9c886672f7837">3.5.1 - MicroShift with Advanced Cluster Management</h1>
    <div class="lead">Manage the MicroShift cluster through Red Hat Advanced Cluster Management (RHACM).</div>
	<h2 id="microshift-with-advanced-cluster-management">MicroShift with Advanced Cluster Management</h2>
<p>Managing through RHACM (Red Hat Advanced Cluster Management) works just like for any other imported managed cluster (see [docs]). However, as secure production deployments don't provide any form of remote access to the cluster via ssh or kubectl, the recommended approach is to define a new cluster with ACM to get managed cluster credentials, then using your device (configuration) management agent of your choice to synchronise those credentials to the device and have MicroShift apply them automatically.</p>
<p>The feature of using RHACM to manage the lifecycle of applications running on MicroShift is only available for AMD64 based systems. Starting with RHACM 2.5, the management functionality of applications running on MicroShift will be available on ARM based architectures.</p>
<p>The steps below assume that RHACM has been installed on a cluster recognized as the hub cluster and that MicroShift is installed on a separate cluster referred to as the managed cluster.</p>
<h3 id="defining-the-managed-cluster-in-hub-cluster">Defining the managed cluster in hub cluster</h3>
<p>The following steps can be performed in the RHACM UI or on the CLI. On the RHACM hub cluster, run the following commands to define the MicroShift cluster as the managed cluster:</p>
<p>NOTE: Ensure you set the CLUSTER_NAME to a unique value that relates to the MicroShift cluster.</p>
<pre tabindex="0"><code>export CLUSTER_NAME=microshift

oc new-project ${CLUSTER_NAME}

oc label namespace ${CLUSTER_NAME} cluster.open-cluster-management.io/managedCluster=${CLUSTER_NAME}
</code></pre><p>Apply the following to define the managed MicroShift cluster.</p>
<pre tabindex="0"><code>cat &lt;&lt;EOF | oc apply -f -
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${CLUSTER_NAME}
spec:
  clusterName: ${CLUSTER_NAME}
  clusterNamespace: ${CLUSTER_NAME}
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
EOF

cat &lt;&lt;EOF | oc apply -f -
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  hubAcceptsClient: true
EOF

</code></pre><p>This will generate a secret named ${CLUSTER_NAME}-import in the ${CLUSTER_NAME} namespace. Extract the <code>import.yaml</code> and the <code>crds.yaml</code> which requires <code>yq</code> to be installed.</p>
<pre tabindex="0"><code>IMPORT=`oc get secret &quot;$CLUSTER_NAME&quot;-import -n &quot;$CLUSTER_NAME&quot; -o jsonpath={.data.import\\.yaml} | base64 --decode'`
IMPORT_KUBECONFIG=$(yq eval-all '. | select(.metadata.name == &quot;bootstrap-hub-kubeconfig&quot;) | .data.kubeconfig' IMPORT)
</code></pre><h3 id="importing-the-managed-microshift-cluster-to-hub-cluster">Importing the managed Microshift cluster to hub cluster</h3>
<p>The importing process can be done automatically by RHACM components running on the hub cluster once the following steps are performed on managed MicroShift cluster. A detailed explanation can be found in <a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/clusters/managing-your-clusters#importing-the-cluster-manual">RHACM documentation</a>.</p>
<h4 id="prepare-the-manifests">Prepare the manifests</h4>
<p>A list of the K8s manifests based on <a href="https://kustomize.io/">Kustomize</a> can be found in <a href="https://github.com/redhat-et/microshift-documentation/content/en/docs/examples/manifests">this repo</a>. This repo contains more than manifests however we will only focus on the <code>manifests</code> folder. Before syncing the manifests to the MicroShift node, the following commands need to be run to render manifests:</p>
<pre tabindex="0"><code>sed -i &quot;s/{{ .clustername }}/${CLUSTER_NAME}/g&quot; manifests/klusterlet.yaml
sed -i &quot;s/{{ .kubeconfig }}/${IMPORT_KUBECONFIG}/g&quot; manifests/klusterlet-kubeconfighub.yaml
</code></pre><h4 id="sync-manifests-to-microshift-node">Sync manifests to MicroShift node</h4>
<p>The next step is to sync manifests to the MicroShift node. MicroShift has the feature of <a href="https://microshift.io/docs/user-documentation/manifests/">auto-applying manifests</a>. Once it finds a <code>kustomization.yaml</code> file in <code>${DATADIR}/manifests</code> (which defaults to <code>/var/lib/microshift/manifests</code>), <code>kubectl apply -k</code> will be run automatically upon start-up. The rendered manifests then need to be synced to <code>${DATADIR}/manifests</code>.</p>
<p>The syncing of manifests to managed Microshift cluster can be done by utilizing any GitOps tool to fetch the Kubernetes Kustomize manifests and put them in the directory described above, e.g. <a href="https://github.com/redhat-et/transmission">Transmission</a> tool can be used to pull updates and apply them transactionally on the ostree-based Linux operating systems.</p>
<h4 id="microshift-auto-applies-those-manifests-to-register-with-the-acm-cluster">MicroShift auto-applies those manifests to register with the ACM cluster</h4>
<p>The cluster now should have all add-ons enabled and be in a READY state within RHACM.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-067fcdf825875018d80f0acbac8fe3f0">3.5.2 - Deploying MicroShift behind Proxy</h1>
    <div class="lead">How to configure the host OS so MicroShift can work behind a proxy.</div>
	<p>When deploying MicroShift behind a proxy, configure the host OS to use the proxy for both yum and CRI-O.</p>
<h3 id="configuring-http-s-proxy-for-yum">Configuring HTTP(S) proxy for yum</h3>
<p>To configure yum to use a proxy, add the following to <code>/etc/yum.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#b8860b">proxy</span><span style="color:#666">=</span>http://<span style="color:#b8860b">$PROXY_SERVER</span>:<span style="color:#b8860b">$PROXY_SERVER</span>
<span style="color:#b8860b">proxy_username</span><span style="color:#666">=</span><span style="color:#b8860b">$PROXY_USER</span>
<span style="color:#b8860b">proxy_password</span><span style="color:#666">=</span><span style="color:#b8860b">$PROXY_PASSWORD</span>
</code></pre></div><h3 id="configuring-http-s-proxy-for-cri-o-or-podman">Configuring HTTP(S) proxy for CRI-O or Podman</h3>
<p>CRI-O and Podman are Go programs that use the built-in <code>net/http</code> package. To use an HTTP(S) proxy you need to set the <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> environment variables and optionally the <code>NO_PROXY</code> variable to exclude a list of hosts from being proxied). For example, add the following to <code>/etc/systemd/system/crio.service.d/00-proxy.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#666">[</span>Service<span style="color:#666">]</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">NO_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;localhost,127.0.0.1,10.42.0.0/16,10.43.0.0/16&#34;</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">HTTP_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;http://</span><span style="color:#b8860b">$PROXY_USER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PASSWORD</span><span style="color:#b44">@</span><span style="color:#b8860b">$PROXY_SERVER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PORT</span><span style="color:#b44">/&#34;</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">HTTPS_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;http://</span><span style="color:#b8860b">$PROXY_USER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PASSWORD</span><span style="color:#b44">@</span><span style="color:#b8860b">$PROXY_SERVER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PORT</span><span style="color:#b44">/&#34;</span>
</code></pre></div><p>Restart CRI-O:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo systemctl restart crio
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc46ef369ec8c5ad7e090b2e0a105779">3.5.3 - Private registries and pull secrets</h1>
    <div class="lead">MicroShift may need access to a private registry. Access can be granted from registry login or from a pull-secret.</div>
	<p>MicroShift may not have the pull secret for the registry that you are trying to use. For example, MicroShift does
not have the pull secret for <code>registry.redhat.io</code>. In order to use this registry, there are a few approaches.</p>
<h3 id="pulling-container-images-from-private-registries">Pulling Container Images From Private Registries</h3>
<h4 id="use-podman-to-authenticate-to-a-registry">Use Podman to Authenticate to a Registry</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">podman login registry.redhat.io
</code></pre></div><p>Once the podman login is complete, MicroShift will be able to pull images from this registry. This approach works across namespaces.</p>
<p>This approach assumes podman is installed. This might not be true for all MicroShift environments. For example,
if MicroShift is installed through RPM, CRI-O will be installed as a dependency, but not podman. In this case,
one can choose to install podman separately, or use other approaches described below.</p>
<h4 id="authenticate-to-a-registry-with-a-pull-secret">Authenticate to a Registry With a Pull-Secret</h4>
<p>The second approach is to create a pull secret, then let the service account use this pull secret. This approach works within a name space. For example, if the pull secret is stored in a json formatted file &quot;<strong>secret</strong>.json&quot;,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># First create the secret in a name space</span>
oc create secret generic my_pull_secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --from-file<span style="color:#666">=</span>.dockerconfigjson<span style="color:#666">=</span>secret.json <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --type<span style="color:#666">=</span>kubernetes.io/dockerconfigjson
</code></pre></div><p>Alternatively, you can use your container manager configuration file to create the secret:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># First create the secret in a name space using our configuration file</span>
oc create secret generic my_pull_secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --from-file<span style="color:#666">=</span>.dockerconfigjson<span style="color:#666">=</span>.docker/config.json <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --type<span style="color:#666">=</span>kubernetes.io/dockerconfigjson
</code></pre></div><p>Finally, we set the secret as default for pulling</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># Then attach the secret to a service account in the name space</span>
oc secrets link default my_pull_secret --for<span style="color:#666">=</span>pull
</code></pre></div><p>Instead of attaching the secret to a service account, one can also specify the pull secret under the pod spec, Refer to <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">this Kubernetes document</a> for more details.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-377f4900168fa9986825e624d7f6f4c4">3.5.4 - Deploy a basic application</h1>
    <div class="lead">MicroShift operates similar to many other Kubernetes providers. This means that you can use the same tools to deploy and manage your applications.</div>
	<p>All of the standard Kubernetes management tools can be used to maintain and modify your MicroShift applications. Below we will show some examples using oc, kustomize, and helm to deploy and maintain applications.</p>
<h2 id="example-applications">Example Applications</h2>
<h3 id="metal-lb">Metal LB</h3>
<p>Metal LB is a load balancer that can be used to route traffic to a number of backends.</p>
<p>Creating the Metal LB namespace and deployment.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml
oc apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml
</code></pre></div><p>Once the components are available, a <code>ConfigMap</code> is required to define the address pool for the load balancer to use.</p>
<p>Create the Metal LB ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">oc create -f - &lt;&lt;EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>metallb-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">config</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    address-pools:
</span><span style="color:#b44;font-style:italic">    - name: default
</span><span style="color:#b44;font-style:italic">      protocol: layer2
</span><span style="color:#b44;font-style:italic">      addresses:
</span><span style="color:#b44;font-style:italic">      - 192.168.1.240-192.168.1.250</span><span style="color:#bbb">    
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span></code></pre></div><p>Now we are able to deploy a test application to verify thing are working as expected.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc create ns <span style="color:#a2f">test</span>
oc create deployment nginx -n <span style="color:#a2f">test</span> --image nginx
</code></pre></div><p>Create a service:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">oc create -f - &lt;&lt;EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metallb.universe.tf/address-pool</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span></code></pre></div><p>Verify the service exists and that an IP address has been assigned.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc get svc -n <span style="color:#a2f">test</span>
NAME    TYPE           CLUSTER-IP      EXTERNAL-IP     PORT<span style="color:#666">(</span>S<span style="color:#666">)</span>        AGE
nginx   LoadBalancer   10.43.183.104   192.168.1.241   80:32434/TCP   29m
</code></pre></div><p>Using your browser you can now access the NGINX application by the <code>EXTERNAL-IP</code> provided by the service.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5fbbf9155094d477254e7a9708a94dfd">3.5.5 - Dynamic Provisioning of PVs</h1>
    <div class="lead">MicroShift storage solution can provision persistent volumes dynamically based on claims.</div>
	<p>MicroShift deploys the <a href="https://github.com/kubevirt/hostpath-provisioner">hostpath provisioner</a> as solution to provide persistent storage to pods. The hostpath provisioner pod mounts the <code>/var/hpvolumes</code> directory in order to provision volumes. It also has the ability to dynamically provision PVs when a PVC is created, and wait until a pod uses that specific PVC.</p>
<p>Let's see how to create a PVC so the hostpath provisioner creates the persistent volume for us.</p>
<h3 id="create-a-persistent-volume-claim">Create a Persistent Volume Claim</h3>
<p>MicroShift's hostpath provisioner creates a <code>StorageClass</code> named <code>kubevirt-hostpath-provisioner</code> by default.</p>
<p>The PVC manifest must reference this <code>StorageClass</code> using the <code>storageClassName</code> spec parameter and there should be an annotation pointing at the node where the PV is going to be created. <em>This annotation is crucial if we want dynamic provisioning of PVs</em>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubevirt.io/provisionOnNode</span>:<span style="color:#bbb"> </span>ricky-fedora.oglok.net<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>kubevirt-hostpath-provisioner<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></code></pre></div><p>This manifest will create the following Persistent Volume Claim and a Persistent Volume located at <code>/var/hpvolumes/</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ oc get pvc
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                    AGE
task-pv-claim   Bound    pvc-58a28c40-7726-4830-ba70-32d18188a8b4   39Gi       RWO            kubevirt-hostpath-provisioner   8m43s
$ oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS                    REASON   AGE
pvc-58a28c40-7726-4830-ba70-32d18188a8b4   39Gi       RWO            Delete           Bound    default/task-pv-claim   kubevirt-hostpath-provisioner            8m43s

$ ll /var/hpvolumes/
total <span style="color:#666">0</span>
drwxrwxrwx. <span style="color:#666">1</span> root root <span style="color:#666">8</span> Apr  <span style="color:#666">5</span> 10:26 pvc-58a28c40-7726-4830-ba70-32d18188a8b4
</code></pre></div><p>For sake of clarity, we will instantiate a sample NGINX pod that mounts that volume:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http-server&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/usr/share/nginx/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span></code></pre></div><p>Any HTML file located at <code>/var/hpvolumes/pvc-58a28c40-7726-4830-ba70-32d18188a8b4</code> can be exposed by the NGINX running in the pod using a regular service.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7c8cb1c1a13c6df4a3f62a4c5665b735">3.5.6 - Offline/disconnected container images</h1>
    <div class="lead">Offline containers are containers which are stored in the operating system image, and made available to <code>cri-o</code> via the <code>/etc/container/storage.conf</code> <code>additionalimagestores</code> list.</div>
	<h2 id="what-are-offline-container-images">What are offline container images</h2>
<p>Offline containers are containers which are stored in the operating, or
the operating system image for an ostree based system,
and made available to <code>cri-o</code> via the <code>/etc/container/storage.conf</code>
<code>additionalimagestores</code> list.</p>
<p>Those container images are accesible for <code>cri-o</code> to create containers. Those images
cannot be deleted, but newer versions of those containers can be downloaded normally,
which <code>cri-o</code> will store in the general R/W container storage of the system.</p>
<h2 id="when-to-use-offline-container-images">When to use offline container images</h2>
<p>Offline containers are useful when the edge device will have restricted connectivity,
or no connectivity at all. Those containers are also helpful to improve general MicroShift
and application startup on first boot, since no images need to be downloaded from the
network and the applications are readily available to <code>cri-o</code></p>
<h2 id="rpm-packaging-of-container-images">RPM packaging of container images</h2>
<p>RPM packaging of container images into read-only container storage is offered via the
<a href="https://github.com/openshift/microshift/blob/main/packaging/rpm/paack.py"><code>paack</code></a> tool
as an experimental method to allow users to create ostree images containing the desired containers.
<code>RPM</code> was not designed for storing files with numeric uids/gids, or containing extended attributes,
although several workarounds allow this we are looking for better ways to provide this.</p>
<h2 id="offline-microshift-containers-images">Offline MicroShift containers images</h2>
<p>MicroShift uses a set of containers for the minimal components which can be installed
on the operating system image, those are published <a href="https://copr.fedorainfracloud.org/coprs/g/redhat-et/microshift-containers/">here</a>, and can also be manually built using:  <code>packaging/rpm/make-microshift-images-rpm.sh</code>.</p>
<p>To install the MicroShift container images you can use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -L -o /etc/yum.repos.d/microshift-containers.repo <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          https://copr.fedorainfracloud.org/coprs/g/redhat-et/microshift-containers/repo/fedora-35/group_redhat-et-microshift-containers-fedora-35.repo

rpm-ostree install microshift-containers
</code></pre></div><p>Or simply include this package when using image-builder.</p>
<h2 id="how-package-your-application-and-manifests-as-rpms-for-offline-container-storage">How package your application and manifests as rpms for offline container storage</h2>
<p>To package workload application container images we provide <code>packaging/rpm/paack.py</code>.
This tool accepts a yaml definition, for which an example can be found
<a href="https://github.com/openshift/microshift/blob/main/packaging/rpm/example-user-containers.yaml">here</a>.</p>
<p>The tool can produce an srpm, rpm, or push a build to a copr repository.</p>
<p>Some example usages:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py rpm example-user-containers.yaml centos-stream-9-aarch64
</code></pre></div><p>The target OS is not important (<code>centos-stream-9</code>) but we need one os target
compatible with the destination architecture.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py srpm example-user-containers.yaml
</code></pre></div><p>The produced <code>srpm</code> format contains the repository binaries and manifests for each architecture,
then the build system unpacks the specific architecture for the build. The post install step
of rpm configures the <code>additionalimagestores</code> in <code>/etc/container/storage.conf</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py copr example-user-containers.yaml mangelajo/my-app-containers
</code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-da827023455bf1040faa66fb1ec83e44">3.6 - Troubleshooting</h1>
    <div class="lead">MicroShift known issues and troubleshooting tips.</div>
	<h2 id="on-ec2-with-rhel-8-4">On EC2 with RHEL 8.4</h2>
<h3 id="service-ca-can-t-be-created">service-ca can't be created</h3>
<p>If you want to run <code>microshift</code> on EC2 RHEL 8.4(<code>cat /etc/os-release</code>), you might find <a href="https://github.com/openshift/microshift/issues/270"><code>ingress and service-ca will not stay online</code></a>.</p>
<p>Inside the failing pods, you might find errors as: <code>10.43.0.1:443: read: connection timed out</code>.</p>
<p>This a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1912236#c30">known issue</a> on RHEL 8.4 and will be resolved in 8.5.</p>
<p>In order to work on RHEL 8.4, you may disable the NetworkManager and reboot to resolve this issue.</p>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">systemctl disable nm-cloud-setup.service nm-cloud-setup.timer
reboot
</code></pre></div><p>You can find the details of this EC2 NetworkManager issue tracked at <a href="https://gitlab.freedesktop.org/NetworkManager/NetworkManager/-/issues/740">issue</a>.</p>
<h3 id="openshift-pods-crashloopbackoff">OpenShift pods CrashLoopBackOff</h3>
<p>A few minutes after <code>microshift</code> started, OpenShift pods fall into <code>CrashLoopBackOff</code>.</p>
<p>If you check up the <code>journalctl |grep iptables</code>, you may see the following:</p>
<pre tabindex="0"><code class="language-log" data-lang="log">
Sep 21 19:12:54 ip-172-31-85-30.ec2.internal microshift[1297]: I0921 19:12:54.399365    1297 server_others.go:185] Using iptables Proxier.
Sep 21 19:13:50 ip-172-31-85-30.ec2.internal kernel: iptables[2438]: segfault at 88 ip 00007feaf5dc0e47 sp 00007fff6f2fea08 error 4 in libnftnl.so.11.3.0[7feaf5dbc000+16000]
Sep 21 19:13:50 ip-172-31-85-30.ec2.internal systemd-coredump[2442]: Process 2438 (iptables) of user 0 dumped core.
Sep 21 20:35:57 ip-172-31-85-30.ec2.internal microshift[1297]: E0921 20:35:57.914558    1297 remote_runtime.go:143] StopPodSandbox &quot;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475&quot; from runtime service failed: rpc error: code = Unknown desc = failed to destroy network for pod sandbox k8s_service-ca-64547678c6-2nxnp_openshift-service-ca_6236deba-fc5f-4915-817d-f8699a4accfc_0(1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475): error removing pod openshift-service-ca_service-ca-64547678c6-2nxnp from CNI network &quot;crio&quot;: running [/usr/sbin/iptables -t nat -D POSTROUTING -s 10.42.0.3 -j CNI-d5d0edec163ce01e4591c1c4 -m comment --comment name: &quot;crio&quot; id: &quot;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475&quot; --wait]: exit status 2: iptables v1.8.4 (nf_tables): Chain 'CNI-d5d0edec163ce01e4591c1c4' does not exist
</code></pre><p>Also, the <code>openshift-ingress</code> pod will fail on:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="color:#888">I0921 17:36:17.811391       1 router.go:262] router &#34;msg&#34;=&#34;router is including routes in all namespaces&#34;
</span><span style="color:#888">E0921 17:36:17.914638       1 haproxy.go:418] can&#39;t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory
</span><span style="color:#888">I0921 17:36:17.948417       1 router.go:579] template &#34;msg&#34;=&#34;router reloaded&#34;  &#34;output&#34;=&#34; - Checking http://localhost:80 ...\n - Health check ok : 0 retry attempt(s).\n&#34;
</span></code></pre></div><p>As a workaround, you can follow steps below:</p>
<ul>
<li>
<p>delete <code>flannel</code> <code>daemonset</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc delete ds -n kube-system kube-flannel-ds
</code></pre></div></li>
<li>
<p>restart all the OpenShift pods.</p>
</li>
</ul>
<p>This workaround won't affect the single node <code>microshift</code> functionality since the <code>flannel</code> <code>daemonset</code> is used for multi-node MicroShift.</p>
<p>This issue is tracked at: <a href="https://github.com/openshift/microshift/issues/296">#296</a></p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1ec80b85799e8d84209d74c8205080a1">4 - Developer Documentation</h1>
    <div class="lead">Building and running MicroShift for local development</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-ee7318597e2a727a11960bedd6dffd3f">4.1 - Building and Running MicroShift</h1>
    <div class="lead">Building and running the MicroShift binary for local development</div>
	<h2 id="system-requirements">System Requirements</h2>
<p>For building MicroShift you need a system with a minimum of</p>
<ul>
<li>a supported 64-bit CPU architecture (amd64/x86_64, arm64, or riscv64)</li>
<li>a supported Linux OS (RHEL 8, CentOS Stream, or Fedora 34+)</li>
<li>2 CPU cores</li>
<li>3GB of RAM</li>
<li>1GB of free storage space for MicroShift</li>
</ul>
<h2 id="building-microshift">Building MicroShift</h2>
<p>Install the build-time dependencies:</p>
<ul class="nav nav-tabs" id="tabset-docs-developer-documentation-local-development-1" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabset-docs-developer-documentation-local-development-1-0" role="tab" aria-controls="tabset-docs-developer-documentation-local-development-1-0" aria-selected="true">RHEL</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabset-docs-developer-documentation-local-development-1-1" role="tab" aria-controls="tabset-docs-developer-documentation-local-development-1-1">Fedora, CentOS Stream</a></li></ul>
<div class="tab-content" id="tabset-docs-developer-documentation-local-development-1"><div id="tabset-docs-developer-documentation-local-development-1-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabset-docs-developer-documentation-local-development-1-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf install -y git make golang
</code></pre></div></div>
  <div id="tabset-docs-developer-documentation-local-development-1-1" class="tab-pane" role="tabpanel" aria-labelledby="tabset-docs-developer-documentation-local-development-1-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf install -y git make golang
</code></pre></div></div></div>

<br/>
<p>Clone the repository and <code>cd</code> into it:
<div class="alert alert-danger warning callout" role="alert">
  <strong></strong> <p>The available community documentation is not currently compatible with the latest MicroShift source code.
To build the latest MicroShift binary, follow the instructions in the <a href="https://github.com/openshift/microshift">openshift/microshift GitHub repository</a>.</p>
<p>Otherwise, use the developer preview RPMs by following the instructions in the <a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12">MicroShift developer preview documentation</a>, using one of the <a href="https://developers.redhat.com/blog/2021/02/10/how-to-activate-your-no-cost-red-hat-enterprise-linux-subscription">no-cost Red Hat Developer subscriptions</a>.</p>

</div>
</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">git clone -b 4.8.0-microshift-2022-04-20-141053 https://github.com/openshift/microshift.git
<span style="color:#a2f">cd</span> microshift
</code></pre></div><p>Build MicroShift:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash"><span style="color:#080;font-style:italic"># release build (without debug symbols)</span>
make

<span style="color:#080;font-style:italic"># development build (with debug symbols)</span>
make <span style="color:#b8860b">DEBUG</span><span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><h2 id="running-microshift">Running MicroShift</h2>
<p>MicroShift requires <code>CRI-O</code> to be installed and running on the host. <br>
Refer to <a href="https://microshift.io/docs/getting-started/#install-cri-o">Getting Started: Install CRI-O</a></p>
<p>Install the SELinux policies from RPM or build and install them from source:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash"><span style="color:#080;font-style:italic"># from RPM</span>
sudo dnf copr <span style="color:#a2f">enable</span> -y @redhat-et/microshift
sudo dnf install -y microshift-selinux

<span style="color:#080;font-style:italic"># from source</span>
<span style="color:#666">(</span><span style="color:#a2f">cd</span> packaging/selinux <span style="color:#666">&amp;&amp;</span> sudo make install<span style="color:#666">)</span>
</code></pre></div><p>Run MicroShift using</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo ./microshift run
</code></pre></div><p>Now switch to a new terminal to access and use this development MicroShift cluster.</p>
<ul>
<li>
<p>To install OpenShift and Kubernetes clients, follow <a href="https://microshift.io/docs/getting-started/#install-clients">Getting Started: Install Clients</a>.</p>
</li>
<li>
<p>To configure the kubeconfig, follow <a href="https://microshift.io/docs/getting-started/#copy-kubeconfig">Getting Started: Copy  Kubeconfig</a>.</p>
</li>
</ul>
<p>It is now possible to run <code>oc</code> or <code>kubectl</code> commands against the MicroShift environment.</p>
<p>Verify that MicroShift is running:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc get pods -A
</code></pre></div><p>Refer to the <a href="https://microshift.io/docs/user-documentation/">MicroShift user documentation</a></p>
<h2 id="cleaning-up">Cleaning Up</h2>
<p>To stop all MicroShift processes and wipe its state run:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo hack/cleanup.sh
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2ecd4c5c186af77b7fb7444586806f93">4.2 - </h1>
    <div class="lead">Building and installing the MicroShift RPMs for local development</div>
	<h2 id="building-the-rpms">Building the RPMs</h2>
<p>MicroShift binary with systemd unit file and the required SELinux submodule can be built as an RPM using <code>make</code> on an RPM-based distribution.</p>
<p>Install the <a href="https://microshift.io/docs/developer-documentation/local-development/#build-dependencies">MicroShift build dependencies</a> and the RPM specific build-time packages.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install -y git golang rpm-build selinux-policy-devel container-selinux
</code></pre></div><p>Clone the repository and cd into it:
<div class="alert alert-danger warning callout" role="alert">
  <strong></strong> <p>The available community documentation is not currently compatible with the latest MicroShift source code.
To build the latest MicroShift RPMs, follow the instructions in the <a href="https://github.com/openshift/microshift">openshift/microshift GitHub repository</a>.</p>
<p>Otherwise, use the developer preview RPMs by following the instructions in the <a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12">MicroShift developer preview documentation</a>, using one of the <a href="https://developers.redhat.com/blog/2021/02/10/how-to-activate-your-no-cost-red-hat-enterprise-linux-subscription">no-cost Red Hat Developer subscriptions</a>.</p>

</div>
</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">git clone -b 4.8.0-microshift-2022-04-20-141053 https://github.com/openshift/microshift.git
<span style="color:#a2f">cd</span> microshift
</code></pre></div><p>Build the SELinux and MicroShift RPMs with:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">make rpm
</code></pre></div><p>RPMs will be placed in <code>./packaging/rpm/_rpmbuild/RPMS/</code>. There are two RPMs that will be required to install:</p>
<pre tabindex="0"><code>packaging/rpm/_rpmbuild/RPMS/noarch/microshift-selinux-*
packaging/rpm/_rpmbuild/RPMS/x86_64/microshift-*
</code></pre><h2 id="installing-the-rpms">Installing the RPMs</h2>
<p>Enable the CRI-O repository:</p>
<ul class="nav nav-tabs" id="tabset-docs-developer-documentation-build-install-rpm-1" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabset-docs-developer-documentation-build-install-rpm-1-0" role="tab" aria-controls="tabset-docs-developer-documentation-build-install-rpm-1-0" aria-selected="true">RHEL</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabset-docs-developer-documentation-build-install-rpm-1-1" role="tab" aria-controls="tabset-docs-developer-documentation-build-install-rpm-1-1">Fedora</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabset-docs-developer-documentation-build-install-rpm-1-2" role="tab" aria-controls="tabset-docs-developer-documentation-build-install-rpm-1-2">CentOS Stream</a></li></ul>
<div class="tab-content" id="tabset-docs-developer-documentation-build-install-rpm-1"><div id="tabset-docs-developer-documentation-build-install-rpm-1-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabset-docs-developer-documentation-build-install-rpm-1-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash"><span style="color:#a2f">command</span> -v subscription-manager &amp;&gt; /dev/null <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    <span style="color:#666">&amp;&amp;</span> subscription-manager repos --enable rhocp-4.8-for-rhel-8-x86_64-rpms
</code></pre></div></div>
  <div id="tabset-docs-developer-documentation-build-install-rpm-1-1" class="tab-pane" role="tabpanel" aria-labelledby="tabset-docs-developer-documentation-build-install-rpm-1-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf module <span style="color:#a2f">enable</span> -y cri-o:1.21
</code></pre></div></div>
  <div id="tabset-docs-developer-documentation-build-install-rpm-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-docs-developer-documentation-build-install-rpm-1-2">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf module <span style="color:#a2f">enable</span> -y cri-o:1.21
</code></pre></div></div></div>

<br/>
<p>Install the MicroShift and the SELinux policies:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf localinstall -y packaging/rpm/_rpmbuild/RPMS/noarch/microshift-selinux-*
sudo dnf localinstall -y packaging/rpm/_rpmbuild/RPMS/x86_64/microshift-*
</code></pre></div><h2 id="running-the-rpms">Running the RPMs</h2>
<p>Start CRI-O and MicroShift services:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo systemctl <span style="color:#a2f">enable</span> crio --now
sudo systemctl <span style="color:#a2f">enable</span> microshift --now
</code></pre></div><p>To install OpenShift and Kubernetes clients, follow <a href="https://microshift.io/docs/getting-started/#install-clients">Getting Started: Install Clients</a>.</p>
<p>To configure the kubeconfig, follow <a href="https://microshift.io/docs/getting-started/#copy-kubeconfig">Getting Started: Copy Kubeconfig</a>.</p>
<p>It is now possible to run <code>oc</code> or <code>kubectl</code> commands against the MicroShift environment.</p>
<p>Verify that MicroShift is running:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc get pods -A
</code></pre></div><p>You can stop MicroShift service with systemd:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl stop microshift
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong></strong> <ul>
<li>cluster data at <code>/var/lib/microshift</code> and <code>/var/lib/kubelet</code>, will not be deleted upon stopping services.
Upon a restart, the cluster state will persist as long as the storage is intact.</li>
</ul>

</div>

<p>Check MicroShift with</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo podman ps
sudo critcl ps
</code></pre></div><p>For more on running MicroShift, refer to the <a href="https://microshift.io/docs/user-documentation/">user documentation</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8ee56599b9f5a3e95a8e3e124395fe92">4.3 - Optional vagrant setup</h1>
    <div class="lead">Vagrant environment for local development (optional)</div>
	<p>It is possible to use Vagrant for VM provisioning, however it is not necessary.</p>
<p>Find a guide on how to install it for your system <a href="https://www.vagrantup.com/downloads">here</a>.</p>
<p>Once Vagrant is installed, create a Vagrant box for the operating
system of choice. For this example we will be looking at a <a href="https://app.vagrantup.com/fedora/boxes/34-cloud-base">fedora 34 cloud
image</a>, however you can substitute any vagrant image of your choice.</p>
<p>First, navigate to the MicroShift directory on your host system, or another designated
directory where we will be storing the <code>Vagrantfile</code>.</p>
<p>Next, download the vagrant image. For this example we will use
a fedora 34 cloud image:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">vagrant box add fedora/34-cloud-base
</code></pre></div><p>Depending on the image, Vagrant will ask you to select a Virtualization provider,
just select the first one.</p>
<p>Once that downloads, initialize the repository for launching your image:</p>
<pre tabindex="0"><code>vagrant init fedora/34-cloud-base
</code></pre><p>Running this command will create a <code>Vagrantfile</code> in your working directory which
is used to configure your vagrant box.</p>
<p>Before starting the Vagrant box, increase the amount of RAM available to the system.
To do this, edit the <code>Vagrantfile</code> and configure your provider settings to include
the following:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb">    config<span style="color:#666">.</span>vm<span style="color:#666">.</span>provider <span style="color:#b44">&#34;libvirt&#34;</span> <span style="color:#a2f;font-weight:bold">do</span> <span style="color:#666">|</span>v<span style="color:#666">|</span>
        <span style="color:#080;font-style:italic"># provides 3GB of memory</span>
        v<span style="color:#666">.</span>memory <span style="color:#666">=</span> <span style="color:#666">3072</span>
        <span style="color:#080;font-style:italic"># for parallelization</span>
        v<span style="color:#666">.</span>cpus <span style="color:#666">=</span> <span style="color:#666">2</span>
    <span style="color:#a2f;font-weight:bold">end</span>
</code></pre></div><p>The value of <code>config.vm.provider</code> depends on the provider you selected when you
ran <code>vagrant add</code> earlier. For example, if you selected <code>virtualbox</code> then the first
line should be: <code>config.vm.provider &quot;virtualbox&quot; do |v|</code></p>
<p>Now start the VM:</p>
<pre tabindex="0"><code>vagrant up
</code></pre><p>Once the VM is up, connect to it:</p>
<pre tabindex="0"><code>vagrant ssh
</code></pre><p>Once ssh'd into the vagrant instance, refer to the <a href="https://microshift.io/docs/developer-documentation/local-development/#build-dependencies">local build and install</a> to begin local development.</p>
<h3 id="extra-optional-connecting-vscode-to-vagrant">(Extra Optional) Connecting VSCode to Vagrant</h3>
<p>If using VSCode, you can connect to your vagrant box with a few extra steps.</p>
<h4 id="increasing-memory-requirements">Increasing Memory Requirements</h4>
<p>Since VSCode leans more on the heavy side of development, the RAM usage on your Vagrant environment
can go up to 5GB, and therefore we will need to modify the <code>Vagrantfile</code> to
increase the amount of available RAM from 3GB to 5GB (or 6GB if you want to be safe).
To do this, set <code>v.memory</code> to the following in your <code>Vagrantfile</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb">        <span style="color:#080;font-style:italic"># provides 5GB of memory</span>
        v<span style="color:#666">.</span>memory <span style="color:#666">=</span> <span style="color:#666">5120</span>
        <span style="color:#080;font-style:italic"># provides 6GB of memory</span>
        v<span style="color:#666">.</span>memory <span style="color:#666">=</span> <span style="color:#666">6144</span>
</code></pre></div><h4 id="setting-up-an-ssh-profile">Setting up an SSH Profile</h4>
<p>First we need to ask Vagrant for an SSH config file. From your host machine, run:</p>
<pre tabindex="0"><code>vagrant ssh-config &gt; ssh-config.conf
</code></pre><p><em>You can edit the <code>ssh-config.conf</code> file to change the hostname from <code>default</code> to
<code>vagrant</code> to be more easily identifiable, but that's up to you. :)</em></p>
<p>Here's an example of a working SSH config file:</p>
<pre tabindex="0"><code>Host default
  HostName 127.0.0.1
  User vagrant
  Port 2222
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /path/to/microshift/.vagrant/machines/default/virtualbox/private_key
  IdentitiesOnly yes
  LogLevel FATAL
</code></pre><p>Next, you'll want to install the <code>Remote - SSH</code> extension from the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh">VSCode Marketplace</a></p>
<p>With the extension installed, you'll click on the green bottom in the bottom-left
corner of VSCode to open a dropdown menu for SSH options:</p>
<p><img src="../../../images/vscode-remote-button.png" alt="VSCode Remote Button"></p>
<p>Select the option to open an SSH configuration file:
<img src="../../../images/remote-ssh-dropdown.png" alt="Dropdown Menu"></p>
<p>Next you'll want to navigate to the &quot;Remote Explorer&quot; tab on the left-hand side
of VSCode, then select on the vagrant target (default if you haven't renamed it)
and click on the button to connect to it in a remote window.</p>
<p><em>(Credits to Andrés Lopez for this guide: <a href="https://medium.com/@lopezgand/connect-visual-studio-code-with-vagrant-in-your-local-machine-24903fb4a9de">Connect Visual Studio Code with Vagrant in your local machine
</a>)</em></p>
<p>Now that the vagrant environment is setup, refer to the <a href="https://microshift.io/docs/developer-documentation/local-development/#build-dependencies">local build and install</a></p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c09f1065b07f2ae3bb609740a3f10dd0">5 - Community</h1>
    <div class="lead">MicroShift community is growing, we hope you can get involved!</div>
	<h2 id="here-s-how-to-get-involved">Here's how to get involved</h2>
<ul>
<li>
<p>Join us on <a href="https://microshift.slack.com">Slack</a>! (<a href="https://join.slack.com/t/microshift/shared_invite/zt-uxncbjbl-XOjueb1ShNP7xfByDxNaaA">Invite to the Slack space</a>)</p>
</li>
<li>
<p>The MicroShift team is incredibly grateful for the public interest the project has gained.  As this interest grows, we feel it will be critical to establish a solid community framework that can scale with community involvement.  Because MicroShift is still in its early days and moving quickly, the team's energy will be focused on driving the project towards its technical goals.  Until such a time that it's decided there is a need for a community framework, please utilize our public slack channel.</p>
</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c9da5865a77748e4db7d4db31200b34f">6 - Release notes</h1>
    <div class="lead">MicroShift release process</div>
	<p>MicroShift is not on a regular release schedule.</p>
<p>You may view the release page <a href="https://github.com/openshift/microshift/releases">here</a></p>
<p>MicroShift is built from <a href="https://www.okd.io/">OKD</a>, the Kubernetes distribution by the OpenShift community. The OKD version currently used to build is <code>4.8.0-0.okd-2021-10-10-030117</code></p>
<div class="alert alert-danger warning callout" role="alert">
  <strong></strong> The available community documentation is not currently compatible with the latest MicroShift source code.
It is recommended to follow the instructions in the <a href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12">MicroShift developer preview documentation</a>, using one of the <a href="https://developers.redhat.com/blog/2021/02/10/how-to-activate-your-no-cost-red-hat-enterprise-linux-subscription">no-cost Red Hat Developer subscriptions</a>.
</div>


</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cb28f35491c0139ab4f6852cc850169f">7 - Differences between MicroShift and OKD</h1>
    <div class="lead">Addressing some innate differences between OKD and MicroShift.</div>
	<h1 id="differences-between-okd-and-microshift">Differences Between OKD and MicroShift</h1>
<p>The design goals behind MicroShift diverge from those of OKD as a necessity of the very different operating environments each project targets.  Namely, OKD achieves the goal of providing a full-stack, self-managed container application platform, targeting developer and operations-centric use cases on cloud infrastructure. MicroShift aims to provide a minimal OpenShift experience on small form factor, often headless devices with as conservative a resource overhead as possible. To further the project's goals, the MicroShift team has reduced OKD's feature set to remove functionality not well suited for edge use cases.</p>
<h2 id="deployment">Deployment</h2>
<p>MicroShift's deployment model differs significantly from OKD's.  <a href="https://github.com/openshift/okd#getting-started">Openshift-install</a> fully automates OKD deployment on cloud or baremetal infrastructure and manages system dependencies and configuration.  It goes a long way to provide a streamlined installation model for users.  MicroShift, being a single binary, can be installed atomically and managed like any other app. The user is expected to have some basic knowledge of package installation tools (dnf, yum, rpm).</p>
<p>The MicroShift <a href="https://microshift.io/docs/getting-started/#install-cri-o">documentation</a> provides a step-by-step recipe for preparing a system and installing the app on supported operating systems.</p>
<h2 id="control-plane-services">Control-Plane Services</h2>
<p>The most notable difference between an OKD and MicroShift runtime are the lack of <a href="https://docs.openshift.com/container-platform/4.8/operators/operator-reference.html#machine-config-operator_platform-operators-ref">operators</a>. Running an operator for each control-plane component becomes quite costly at the edge.  Operators are built on the <a href="https://operatorframework.io/">operator-framework</a>, which provides a wonderful toolset and boilerplate for orchestrating application lifecycle management.  In order to reduce the platform's resource footprint, MicroShift compiles the control-plane into a single binary. This architecture means that the control-plane applications are not managed through the Kubernetes API, making the role of their operators moot.  The result of this design is a measurable reduction of redundant code (operator boilerplate) and a lower runtime overhead.  For most cases, we do not expect this to impact application portability.</p>
<h4 id="embedded-services">Embedded Services</h4>
<h5 id="control-plane">Control-Plane</h5>
<ul>
<li>etcd</li>
<li>kube-scheduler</li>
<li>kube-apiserver</li>
<li>openshift-api-server</li>
<li>kube-controller-manager</li>
<li>openshift-controller-manager</li>
<li>openshift-oath</li>
<li><a href="https://github.com/openshift/microshift/pull/429">multicast dns</a> (for multi-node enablement)</li>
</ul>
<h5 id="node">Node</h5>
<ul>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<h4 id="deployed-services">Deployed Services</h4>
<p>Post boot, MicroShift deploys the following services:</p>
<ul>
<li>openshift-service-ca</li>
<li>openshift-ingress</li>
<li>kubevirt-hostpath-provisioner</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      <a class="text-white" href="/docs/home/">Home</a>
      
      <a class="text-white" href="/docs/community/">Community</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/microshift_io">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=nj6l882mfe4d2g9nr1h7avgrcs%40group.calendar.google.com&amp;ctz=America%2FChicago">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://www.youtube.com/results?search_query=MicroShift&#43;Red&#43;Hat">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/openshift/microshift">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://microshift.slack.com">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2025 </small>
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>











<script src="/js/main.min.d450479ebf922238b0349336733304a8093ef200121614f1214a9d106d3775d0.js" integrity="sha256-1FBHnr&#43;SIjiwNJM2czMEqAk&#43;8gASFhTxIUqdEG03ddA=" crossorigin="anonymous"></script>








<script type="text/javascript" src="/js/jquery-ui.min.c31ea7e64c1740bcef4b5af6b26f61e5b19ab33388395bc56d4ea49229c5b84d.js"></script>

<script>$( function() { $( ".tabs" ).tabs();} ); </script>
  </body>
</html>
