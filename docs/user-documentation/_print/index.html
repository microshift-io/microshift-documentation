<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.89.4" />

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<link rel="canonical" type="text/html" href="https://microshift.io/docs/user-documentation/">
<link rel="apple-touch-icon" sizes="180x180" href="/images/logo/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/logo/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/logo/favicon-16x16.png">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="mask-icon" href="images/logo/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#00aba9">
<meta name="theme-color" content="#ffffff">

<title>User Documentation | MicroShift</title><meta property="og:title" content="User Documentation" />
<meta property="og:description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://microshift.io/docs/user-documentation/" /><meta property="og:site_name" content="MicroShift" />

<meta itemprop="name" content="User Documentation">
<meta itemprop="description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going."><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="User Documentation"/>
<meta name="twitter:description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going."/>





<link rel="preload" href="/scss/main.min.74383d8b2c66513431ce06d78f9cea3f5745dd97194594c5acd57ec8cc1c401c.css" as="style">
<link href="/scss/main.min.74383d8b2c66513431ce06d78f9cea3f5745dd97194594c5acd57ec8cc1c401c.css" rel="stylesheet" integrity="">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<meta name="theme-color" content="#326ce5">






<meta name="description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going.">
<meta property="og:description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going.">
<meta name="twitter:description" content="After MicroShift is up and running, what&#39;s next? Follow the user documentation to explore. Here you&#39;ll find a section of `HowTos` to get you going.">
<meta property="og:url" content="https://microshift.io/docs/user-documentation/">
<meta property="og:title" content="User Documentation">
<meta name="twitter:title" content="User Documentation">
<meta name="twitter:image" content="/images/favicon.png" />

<meta name="twitter:image:alt" content="MicroShift">

<meta property="og:image" content="/images/favicon.png">

<meta property="og:type" content="article">



<script src="/js/script.js"></script>




<link rel="stylesheet" href="/css/jquery-ui.min.137b1c829b79d35bbaea89c94a60a56165e443ca5dd258afed9f60317c44cc98.css">


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">

		<ul class="navbar-nav mt-2 mt-lg-0">
			
			<li class="nav-item mr-2 mb-lg-0">
				<a class="nav-link active" href="https://github.com/microshift-io/microshift" >Getting Started</span></a>
			</li>
			<li class="nav-item mr-2 mb-lg-0">
				<a class="nav-link active" href="https://github.com/microshift-io/microshift/blob/main/README.md" >Documentation</span></a>
			</li>
			
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/docs/user-documentation/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">User Documentation</h1>
<div class="lead">After MicroShift is up and running, what's next? Follow the user documentation to explore. Here you'll find a section of <code>HowTos</code> to get you going.</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-cd239422cb5c418ee995f07d941d6bd6">Disconnected deployment</a></li>


    
  
    
    
	
<li>2: <a href="#pg-0d2b3789b669c1012e6e8f251fcef6f0">Configuring MicroShift</a></li>


    
  
    
    
	
<li>3: <a href="#pg-107e4cf533a2ff324bdac20c14bdf742">Auto-applying Manifests</a></li>


    
  
    
    
	
<li>4: <a href="#pg-49c842618222bb9ce6ca9ca10964aa29">Networking</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-b0e7cd97514654edfa7e1b5bff515294">Overview</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-93d339436d3fd951d49aab2c806ac37e">Exposing Services</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-a813f4a2f155401b0415650bb22eb277">Firewall</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-0477370f872d1351ebe6eea051d22d47">mDNS</a></li>


    
  
    
    
	
<li>4.5: <a href="#pg-1e1613f966091eff100b2129a8472a68">CNI Plugin</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-53680479178b1a1c2ff6d5d8d000797c">HowTos</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-c49e8fdeb5e48b453ab9c886672f7837">MicroShift with Advanced Cluster Management</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-067fcdf825875018d80f0acbac8fe3f0">Deploying MicroShift behind Proxy</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-bc46ef369ec8c5ad7e090b2e0a105779">Private registries and pull secrets</a></li>


    
  
    
    
	
<li>5.4: <a href="#pg-377f4900168fa9986825e624d7f6f4c4">Deploy a basic application</a></li>


    
  
    
    
	
<li>5.5: <a href="#pg-5fbbf9155094d477254e7a9708a94dfd">Dynamic Provisioning of PVs</a></li>


    
  
    
    
	
<li>5.6: <a href="#pg-7c8cb1c1a13c6df4a3f62a4c5665b735">Offline/disconnected container images</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-da827023455bf1040faa66fb1ec83e44">Troubleshooting</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-cd239422cb5c418ee995f07d941d6bd6">1 - Disconnected deployment</h1>
    <div class="lead">MicroShift can run without internet connectivity.</div>
	<h2 id="wip-content-coming-soon">WIP Content coming soon</h2>
<p>Pre-load MicroShift image tarball into CRI-O</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0d2b3789b669c1012e6e8f251fcef6f0">2 - Configuring MicroShift</h1>
    <div class="lead">Configuration options with MicroShift</div>
	<h2 id="configuration">Configuration</h2>
<p>Microshift can be configured in three simple ways, in order of precedence:</p>
<ul>
<li>Commandline arguments</li>
<li>Environment variables</li>
<li>Configuration file</li>
</ul>
<p>An example configuration can be found <a href="https://github.com/openshift/microshift/blob/main/test/config.yaml">here</a>.</p>
<p>Below is a table of consisting of the configuration settings presently offered in MicroShift, along with the ways they can be set, what they mean, and their default values.</p>
<table>
<thead>
<tr>
<th>MicroshiftConfig field</th>
<th>CLI Argument</th>
<th>Environment Variable</th>
<th>Configuration File</th>
<th>Meaning</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataDir</code></td>
<td><code>--data-dir</code></td>
<td><code>MICROSHIFT_DATADIR</code></td>
<td><code>.dataDir</code></td>
<td>Data directory for MicroShift</td>
<td><code>&quot;~/.microshift/data&quot;</code></td>
</tr>
<tr>
<td><code>LogDir</code></td>
<td><code>--log-dir</code></td>
<td><code>MICROSHIFT_LOGDIR</code></td>
<td><code>.logDir</code></td>
<td>Directory to output logfiles to</td>
<td><code>&quot;&quot;</code></td>
</tr>
<tr>
<td><code>LogVLevel</code></td>
<td><code>--v</code></td>
<td><code>MICROSHIFT_LOGVLEVEL</code></td>
<td><code>.logVLevel</code></td>
<td>Log verbosity level</td>
<td><code>0</code></td>
</tr>
<tr>
<td><code>LogVModule</code></td>
<td><code>--vmodule</code></td>
<td><code>MICROSHIFT_LOGVMODULE</code></td>
<td><code>.logVModule</code></td>
<td>Log verbosity module</td>
<td><code>&quot;&quot;</code></td>
</tr>
<tr>
<td><code>LogAlsotostderr</code></td>
<td><code>--alsologtostderr</code></td>
<td><code>MICROSHIFT_LOGALSOTOSTDERR</code></td>
<td><code>.logAlsotostderr</code></td>
<td>Log into standard error as well</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>Roles</code></td>
<td><code>--roles</code></td>
<td><code>MICROSHIFT_ROLES</code></td>
<td><code>.roles</code></td>
<td>Roles available on the cluster</td>
<td><code>[&quot;controlplane&quot;, &quot;node&quot;]</code></td>
</tr>
<tr>
<td><code>NodeName</code></td>
<td><code>n/a</code></td>
<td><code>MICROSHIFT_NODENAME</code></td>
<td><code>.nodeName</code></td>
<td>Name of the node to run MicroShift on</td>
<td><code>os.Hostname()</code></td>
</tr>
<tr>
<td><code>NodeIP</code></td>
<td><code>n/a</code></td>
<td><code>MICROSHIFT_NODEIP</code></td>
<td><code>.nodeIP</code></td>
<td>Node's IP</td>
<td><code>util.GetHostIP()</code></td>
</tr>
<tr>
<td><code>Cluster.URL</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.url</code></td>
<td>URL that the cluster will run on</td>
<td><code>&quot;https://127.0.0.1:6443&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.ClusterCIDR</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.clusterCIDR</code></td>
<td>Cluster's CIDR</td>
<td><code>&quot;10.42.0.0/16&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.ServiceCIDR</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.serviceCIDR</code></td>
<td>Service CIDR</td>
<td><code>&quot;10.43.0.0/16&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.DNS</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.dns</code></td>
<td>Cluster's DNS server</td>
<td><code>&quot;10.43.0.10&quot;</code></td>
</tr>
<tr>
<td><code>Cluster.Domain</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td><code>.cluster.domain</code></td>
<td>Cluster's domain</td>
<td><code>&quot;cluster.local&quot;</code></td>
</tr>
<tr>
<td><code>ConfigFile</code></td>
<td><code>--config</code></td>
<td><code>n/a</code></td>
<td><code>n/a</code></td>
<td>Path to a config file used to populate the rest of the values</td>
<td><code>&quot;~/.microshift/config.yaml&quot;</code> if the file exists, else <code>/etc/microshift/config.yaml</code> if it exists, else <code>&quot;&quot;</code></td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-107e4cf533a2ff324bdac20c14bdf742">3 - Auto-applying Manifests</h1>
    <div class="lead">Automatically applying manifests for bootstrapping cluster services.</div>
	<p>A common use case after bringing up a new cluster is applying manifests for bootstrapping a <a href="https://microshift.io/docs/user-documentation/how-tos/acm-with-microshift/">management agent like the Open Cluster Management's klusterlet</a> or for starting up services when running disconnected.</p>
<p>MicroShift leverages <a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/"><code>kustomize</code></a> for Kubernetes-native templating and declarative management of resource objects. Upon start-up, it searches <code>/etc/microshift/manifests</code>, <code>/usr/lib/microshift/manifests</code> and
<code>${DATADIR}/manifests</code> (which defaults to <code>/var/lib/microshift/manifests</code>) for a <code>kustomization.yaml</code> file. If it finds one, it automatically runs <code>kubectl apply -k</code> to that kustomization`</p>
<p>Example:</p>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt;/etc/microshift/manifests/nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: NGINX_IMAGE
        ports:
        - containerPort: 8080
EOF

cat &lt;&lt;EOF &gt;/etc/microshift/manifests/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - nginx.yaml
images:
  - name: NGINX_IMAGE
    newName: nginx:1.21
EOF
</code></pre><p>The reason for providing multiple directories is to allow a flexible method to manage
MicroShift workloads.</p>
<table>
<thead>
<tr>
<th>Location</th>
<th>Intent</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/etc/microshift/manifests</code></td>
<td>R/W location for configuration management systems or development</td>
</tr>
<tr>
<td><code>/usr/lib/microshift/manifests</code></td>
<td>RO location, for embedding configuration manifests on ostree based systems</td>
</tr>
<tr>
<td><code>${DATADIR}/manifests</code></td>
<td>R/W location for backwards compatibility (deprecated)</td>
</tr>
</tbody>
</table>
<p>The list of manifest locations can be customized via configuration using the manifests section (see
<a href="https://github.com/openshift/microshift/blob/main/test/config.yaml">here</a>) or via the <code>MICROSHIFT_MANIFESTS</code>
environment variable as comma separated directories.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-49c842618222bb9ce6ca9ca10964aa29">4 - Networking</h1>
    <div class="lead">Understanding and configuring networking in MicroShift.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-b0e7cd97514654edfa7e1b5bff515294">4.1 - Overview</h1>
    <div class="lead">Overview of the MicroShift networking.</div>
	<p>MicroShift uses the host configured networking, either statically configured
or via DHCP. In the case of dynamic addresses MicroShift will restart if an
IP change is detected during runtime.</p>
<p>Connectivity to the K8s API endpoint is provided in the default 6443 port on
the master host(s) IP addresses. If other services in the network must interact
with the MicroShift API connectivity should be performed in any of the following
ways:</p>
<ul>
<li>DNS discovery, pre-configured on the network servers.</li>
<li>Direct IP address connectivity.</li>
<li>mDNS discovery via .local domain, see <a href="../mdns">mDNS</a></li>
</ul>
<p>Conectivity between Pods is handled by the <a href="../cni">CNI</a> plugin on the Pod network
range which defaults to <code>10.42.0.0/16</code> which can be modified via <code>Cluster.ClusterCIDR</code>
<a href="../../configuring/">configuration</a> parameter, see more details in the corresponding sections.</p>
<p>Connectivity to services of type ClusterIP is provided by the embedded <code>kube-proxy</code>
iptables-based implementation on the <code>10.43.0.0/16</code> range which can be modified via
<code>Cluster.ServiceCIDR</code> <a href="../../configuring/">configuration</a> parameter.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-93d339436d3fd951d49aab2c806ac37e">4.2 - Exposing Services</h1>
    <div class="lead">Exposing services in MicroShift.</div>
	<p>Services deployed in MicroShift can be exposed in multiple ways.</p>
<h1 id="routes-and-ingresses">Routes and Ingresses</h1>
<p>By default an OpenShift router is created and exposed on host network ports 80/443.
<code>Routes</code>or <code>Ingresses</code> can be used to expose HTTP or HTTPS services through the router.</p>
<h3 id="example">Example</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc create deployment nginx --image<span style="color:#666">=</span>nginxinc/nginx-unprivileged:stable-alpine
oc expose deployment nginx --port<span style="color:#666">=</span><span style="color:#666">8080</span>
oc expose service/nginx --hostname<span style="color:#666">=</span>my-hostname.com

<span style="color:#080;font-style:italic"># assuming my-hostname.com being mapped to the MicroShift node IP</span>
curl http://my-hostname.com
</code></pre></div><h3 id="route-with-mdns-host-example">Route with mDNS host example</h3>
<p>The hostname of a route can be a mDNS (.local) hostname, which would be then
announced via mDNS, see the <a href="../mdns/">mDNS</a> section for more details.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc expose service/nginx --hostname<span style="color:#666">=</span>my-hostname.local
curl http://my-hostname.local
</code></pre></div><h1 id="service-of-type-nodeport">Service of type NodePort</h1>
<p>NodePort type of services expose services over a dedicated port on all the cluster
nodes, such port is routed internally to the active pods backing the service.</p>
<h3 id="example-1">Example</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc create deployment nginx --image<span style="color:#666">=</span>nginxinc/nginx-unprivileged:stable-alpine
oc expose deployment nginx --type<span style="color:#666">=</span>NodePort --name<span style="color:#666">=</span>nodeport-nginx --port <span style="color:#666">8080</span>
<span style="color:#b8860b">NODEPORT</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>oc get service nodeport-nginx -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.ports[0].nodePort}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#b8860b">IP</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>oc get node -A -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[0].status.addresses[0].address}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
curl http://<span style="color:#b8860b">$IP</span>:<span style="color:#b8860b">$NODEPORT</span>/
</code></pre></div><p>For using <code>NodePort</code> services open the 30000-32767 port range , see the
<a href="../firewall/">firewall</a> section.</p>
<h1 id="service-of-type-loadbalancer">Service of type LoadBalancer</h1>
<p>Services of type <code>LoadBalancer</code> are not supported yet, this kind of service is normally backed
by a load balancer in the underlying cloud.</p>
<p>Multiple alternatives are being explored to provide LoadBalancer VIPs on the LAN.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a813f4a2f155401b0415650bb22eb277">4.3 - Firewall</h1>
    <div class="lead">Firewall considerations for MicroShift</div>
	<p>MicroShift does not require a firewall to run, but it's recommended. In the case of
using firewalld the following ports should be considered:</p>
<table>
<thead>
<tr>
<th>Port(s)</th>
<th>Protocol(s)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>80</td>
<td><code>TCP</code></td>
<td>HTTP port used to serve applications through the OpenShift router.</td>
</tr>
<tr>
<td>443</td>
<td><code>TCP</code></td>
<td>HTTPS port used to serve applications through the OpenShift router.</td>
</tr>
<tr>
<td>6443</td>
<td><code>TCP</code></td>
<td>HTTPS API port for the MicroShift API</td>
</tr>
<tr>
<td>5353</td>
<td><code>UDP</code></td>
<td><a href="../mdns/">mDNS</a> service to respond for OpenShift route mDNS hosts</td>
</tr>
<tr>
<td>30000-32767</td>
<td><code>TCP/UDP</code></td>
<td>Port range reserved for NodePort type of services, can be used to expose applications on the LAN</td>
</tr>
</tbody>
</table>
<p>Additionally pods need to be able to contact the internal coreDNS server, a way
to allow such connectivity is the following, assuming the PodIP range is <code>10.42.0.0/16</code></p>
<p><code>sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16</code></p>
<h2 id="firewalld">Firewalld</h2>
<p>An example for enabling firewalld and opening all the above mentioned ports is:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">sudo dnf install -y firewalld
sudo systemctl <span style="color:#a2f">enable</span> firewalld --now
sudo firewall-cmd --zone<span style="color:#666">=</span>trusted --add-source<span style="color:#666">=</span>10.42.0.0/16 --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>80/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>443/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>6443/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>5353/udp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>30000-32767/tcp --permanent
sudo firewall-cmd --zone<span style="color:#666">=</span>public --add-port<span style="color:#666">=</span>30000-32767/udp --permanent
sudo firewall-cmd --reload
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0477370f872d1351ebe6eea051d22d47">4.4 - mDNS</h1>
    <div class="lead">embedded Multicast DNS support in MicroShift.</div>
	<p>MicroShift includes an embedded mDNS server for deployment scenarios in which the authoritative DNS server cannot be reconfigured to point clients to services on MicroShift.</p>
<p>mDNS is a protocol used to allow name resolution and service discovery within a LAN using
multicast exposed on the <code>5353/UDP</code> port.</p>
<p>This allows <code>.local</code> domains exposed by MicroShift to be discovered by other elements
on the Local Area Network.</p>
<h2 id="notes-for-linux">Notes for Linux</h2>
<p>mDNS resolution on Linux is provided by the avahi-daemon. For other Linux hosts to discover
MicroShift services or for workers to locate the master node via mDNS, avahi should be enabled:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install -y nss-mdns avahi
hostnamectl set-hostname microshift-vm.local
systemctl <span style="color:#a2f">enable</span> --now avahi-daemon.service
</code></pre></div><p>By default only the minimal IPv4 mDNS resolver is implemented, that will only resolve TLD mDNS
domains like <code>hostname.local</code>, if you want to use hostnames in the form of <code>subdomain.domain.local</code>
you need to enable the full mDNS resolver on the host trying to resolve those dns entries:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> .local &gt; /etc/mdns.allow
<span style="color:#a2f">echo</span> .local. &gt;&gt; /etc/mdns.allow
sed -i <span style="color:#b44">&#39;s/mdns4_minimal/mdns/g&#39;</span> /etc/nsswitch.conf
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1e1613f966091eff100b2129a8472a68">4.5 - CNI Plugin</h1>
    <div class="lead">The CNI Plugin used in MicroShift.</div>
	<p>MicroShift uses the <a href="https://github.com/flannel-io/flannel">Flannel</a> CNI network plugin
as lightweight (but less featureful) alternative to OpenShiftSDN or OVNKubernetes.</p>
<p>This provides worker node to worker node pod connectivity via vxlan tunnels.</p>
<p>For single node operation the crio-bridge plugin could be used for additional
resource saving.</p>
<!-- TO-DO: test and documment network-pluging switching -->
<p>Both flannel and crio-bridge have no support for <em>NetworkPolicy</em>.</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53680479178b1a1c2ff6d5d8d000797c">5 - HowTos</h1>
    <div class="lead">Solving common use cases with MicroShift.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c49e8fdeb5e48b453ab9c886672f7837">5.1 - MicroShift with Advanced Cluster Management</h1>
    <div class="lead">Manage the MicroShift cluster through Red Hat Advanced Cluster Management (RHACM).</div>
	<h2 id="microshift-with-advanced-cluster-management">MicroShift with Advanced Cluster Management</h2>
<p>Managing through RHACM (Red Hat Advanced Cluster Management) works just like for any other imported managed cluster (see [docs]). However, as secure production deployments don't provide any form of remote access to the cluster via ssh or kubectl, the recommended approach is to define a new cluster with ACM to get managed cluster credentials, then using your device (configuration) management agent of your choice to synchronise those credentials to the device and have MicroShift apply them automatically.</p>
<p>The feature of using RHACM to manage the lifecycle of applications running on MicroShift is only available for AMD64 based systems. Starting with RHACM 2.5, the management functionality of applications running on MicroShift will be available on ARM based architectures.</p>
<p>The steps below assume that RHACM has been installed on a cluster recognized as the hub cluster and that MicroShift is installed on a separate cluster referred to as the managed cluster.</p>
<h3 id="defining-the-managed-cluster-in-hub-cluster">Defining the managed cluster in hub cluster</h3>
<p>The following steps can be performed in the RHACM UI or on the CLI. On the RHACM hub cluster, run the following commands to define the MicroShift cluster as the managed cluster:</p>
<p>NOTE: Ensure you set the CLUSTER_NAME to a unique value that relates to the MicroShift cluster.</p>
<pre tabindex="0"><code>export CLUSTER_NAME=microshift

oc new-project ${CLUSTER_NAME}

oc label namespace ${CLUSTER_NAME} cluster.open-cluster-management.io/managedCluster=${CLUSTER_NAME}
</code></pre><p>Apply the following to define the managed MicroShift cluster.</p>
<pre tabindex="0"><code>cat &lt;&lt;EOF | oc apply -f -
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${CLUSTER_NAME}
spec:
  clusterName: ${CLUSTER_NAME}
  clusterNamespace: ${CLUSTER_NAME}
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
EOF

cat &lt;&lt;EOF | oc apply -f -
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  hubAcceptsClient: true
EOF

</code></pre><p>This will generate a secret named ${CLUSTER_NAME}-import in the ${CLUSTER_NAME} namespace. Extract the <code>import.yaml</code> and the <code>crds.yaml</code> which requires <code>yq</code> to be installed.</p>
<pre tabindex="0"><code>IMPORT=`oc get secret &quot;$CLUSTER_NAME&quot;-import -n &quot;$CLUSTER_NAME&quot; -o jsonpath={.data.import\\.yaml} | base64 --decode'`
IMPORT_KUBECONFIG=$(yq eval-all '. | select(.metadata.name == &quot;bootstrap-hub-kubeconfig&quot;) | .data.kubeconfig' IMPORT)
</code></pre><h3 id="importing-the-managed-microshift-cluster-to-hub-cluster">Importing the managed Microshift cluster to hub cluster</h3>
<p>The importing process can be done automatically by RHACM components running on the hub cluster once the following steps are performed on managed MicroShift cluster. A detailed explanation can be found in <a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/clusters/managing-your-clusters#importing-the-cluster-manual">RHACM documentation</a>.</p>
<h4 id="prepare-the-manifests">Prepare the manifests</h4>
<p>A list of the K8s manifests based on <a href="https://kustomize.io/">Kustomize</a> can be found in <a href="https://github.com/redhat-et/microshift-documentation/content/en/docs/examples/manifests">this repo</a>. This repo contains more than manifests however we will only focus on the <code>manifests</code> folder. Before syncing the manifests to the MicroShift node, the following commands need to be run to render manifests:</p>
<pre tabindex="0"><code>sed -i &quot;s/{{ .clustername }}/${CLUSTER_NAME}/g&quot; manifests/klusterlet.yaml
sed -i &quot;s/{{ .kubeconfig }}/${IMPORT_KUBECONFIG}/g&quot; manifests/klusterlet-kubeconfighub.yaml
</code></pre><h4 id="sync-manifests-to-microshift-node">Sync manifests to MicroShift node</h4>
<p>The next step is to sync manifests to the MicroShift node. MicroShift has the feature of <a href="https://microshift.io/docs/user-documentation/manifests/">auto-applying manifests</a>. Once it finds a <code>kustomization.yaml</code> file in <code>${DATADIR}/manifests</code> (which defaults to <code>/var/lib/microshift/manifests</code>), <code>kubectl apply -k</code> will be run automatically upon start-up. The rendered manifests then need to be synced to <code>${DATADIR}/manifests</code>.</p>
<p>The syncing of manifests to managed Microshift cluster can be done by utilizing any GitOps tool to fetch the Kubernetes Kustomize manifests and put them in the directory described above, e.g. <a href="https://github.com/redhat-et/transmission">Transmission</a> tool can be used to pull updates and apply them transactionally on the ostree-based Linux operating systems.</p>
<h4 id="microshift-auto-applies-those-manifests-to-register-with-the-acm-cluster">MicroShift auto-applies those manifests to register with the ACM cluster</h4>
<p>The cluster now should have all add-ons enabled and be in a READY state within RHACM.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-067fcdf825875018d80f0acbac8fe3f0">5.2 - Deploying MicroShift behind Proxy</h1>
    <div class="lead">How to configure the host OS so MicroShift can work behind a proxy.</div>
	<p>When deploying MicroShift behind a proxy, configure the host OS to use the proxy for both yum and CRI-O.</p>
<h3 id="configuring-http-s-proxy-for-yum">Configuring HTTP(S) proxy for yum</h3>
<p>To configure yum to use a proxy, add the following to <code>/etc/yum.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#b8860b">proxy</span><span style="color:#666">=</span>http://<span style="color:#b8860b">$PROXY_SERVER</span>:<span style="color:#b8860b">$PROXY_SERVER</span>
<span style="color:#b8860b">proxy_username</span><span style="color:#666">=</span><span style="color:#b8860b">$PROXY_USER</span>
<span style="color:#b8860b">proxy_password</span><span style="color:#666">=</span><span style="color:#b8860b">$PROXY_PASSWORD</span>
</code></pre></div><h3 id="configuring-http-s-proxy-for-cri-o-or-podman">Configuring HTTP(S) proxy for CRI-O or Podman</h3>
<p>CRI-O and Podman are Go programs that use the built-in <code>net/http</code> package. To use an HTTP(S) proxy you need to set the <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> environment variables and optionally the <code>NO_PROXY</code> variable to exclude a list of hosts from being proxied). For example, add the following to <code>/etc/systemd/system/crio.service.d/00-proxy.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#666">[</span>Service<span style="color:#666">]</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">NO_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;localhost,127.0.0.1,10.42.0.0/16,10.43.0.0/16&#34;</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">HTTP_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;http://</span><span style="color:#b8860b">$PROXY_USER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PASSWORD</span><span style="color:#b44">@</span><span style="color:#b8860b">$PROXY_SERVER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PORT</span><span style="color:#b44">/&#34;</span>
<span style="color:#b8860b">Environment</span><span style="color:#666">=</span><span style="color:#b8860b">HTTPS_PROXY</span><span style="color:#666">=</span><span style="color:#b44">&#34;http://</span><span style="color:#b8860b">$PROXY_USER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PASSWORD</span><span style="color:#b44">@</span><span style="color:#b8860b">$PROXY_SERVER</span><span style="color:#b44">:</span><span style="color:#b8860b">$PROXY_PORT</span><span style="color:#b44">/&#34;</span>
</code></pre></div><p>Restart CRI-O:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo systemctl restart crio
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc46ef369ec8c5ad7e090b2e0a105779">5.3 - Private registries and pull secrets</h1>
    <div class="lead">MicroShift may need access to a private registry. Access can be granted from registry login or from a pull-secret.</div>
	<p>MicroShift may not have the pull secret for the registry that you are trying to use. For example, MicroShift does
not have the pull secret for <code>registry.redhat.io</code>. In order to use this registry, there are a few approaches.</p>
<h3 id="pulling-container-images-from-private-registries">Pulling Container Images From Private Registries</h3>
<h4 id="use-podman-to-authenticate-to-a-registry">Use Podman to Authenticate to a Registry</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">podman login registry.redhat.io
</code></pre></div><p>Once the podman login is complete, MicroShift will be able to pull images from this registry. This approach works across namespaces.</p>
<p>This approach assumes podman is installed. This might not be true for all MicroShift environments. For example,
if MicroShift is installed through RPM, CRI-O will be installed as a dependency, but not podman. In this case,
one can choose to install podman separately, or use other approaches described below.</p>
<h4 id="authenticate-to-a-registry-with-a-pull-secret">Authenticate to a Registry With a Pull-Secret</h4>
<p>The second approach is to create a pull secret, then let the service account use this pull secret. This approach works within a name space. For example, if the pull secret is stored in a json formatted file &quot;<strong>secret</strong>.json&quot;,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># First create the secret in a name space</span>
oc create secret generic my_pull_secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --from-file<span style="color:#666">=</span>.dockerconfigjson<span style="color:#666">=</span>secret.json <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --type<span style="color:#666">=</span>kubernetes.io/dockerconfigjson
</code></pre></div><p>Alternatively, you can use your container manager configuration file to create the secret:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># First create the secret in a name space using our configuration file</span>
oc create secret generic my_pull_secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --from-file<span style="color:#666">=</span>.dockerconfigjson<span style="color:#666">=</span>.docker/config.json <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --type<span style="color:#666">=</span>kubernetes.io/dockerconfigjson
</code></pre></div><p>Finally, we set the secret as default for pulling</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># Then attach the secret to a service account in the name space</span>
oc secrets link default my_pull_secret --for<span style="color:#666">=</span>pull
</code></pre></div><p>Instead of attaching the secret to a service account, one can also specify the pull secret under the pod spec, Refer to <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">this Kubernetes document</a> for more details.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-377f4900168fa9986825e624d7f6f4c4">5.4 - Deploy a basic application</h1>
    <div class="lead">MicroShift operates similar to many other Kubernetes providers. This means that you can use the same tools to deploy and manage your applications.</div>
	<p>All of the standard Kubernetes management tools can be used to maintain and modify your MicroShift applications. Below we will show some examples using oc, kustomize, and helm to deploy and maintain applications.</p>
<h2 id="example-applications">Example Applications</h2>
<h3 id="metal-lb">Metal LB</h3>
<p>Metal LB is a load balancer that can be used to route traffic to a number of backends.</p>
<p>Creating the Metal LB namespace and deployment.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml
oc apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml
</code></pre></div><p>Once the components are available, a <code>ConfigMap</code> is required to define the address pool for the load balancer to use.</p>
<p>Create the Metal LB ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">oc create -f - &lt;&lt;EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>metallb-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">config</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    address-pools:
</span><span style="color:#b44;font-style:italic">    - name: default
</span><span style="color:#b44;font-style:italic">      protocol: layer2
</span><span style="color:#b44;font-style:italic">      addresses:
</span><span style="color:#b44;font-style:italic">      - 192.168.1.240-192.168.1.250</span><span style="color:#bbb">    
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span></code></pre></div><p>Now we are able to deploy a test application to verify thing are working as expected.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc create ns <span style="color:#a2f">test</span>
oc create deployment nginx -n <span style="color:#a2f">test</span> --image nginx
</code></pre></div><p>Create a service:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">oc create -f - &lt;&lt;EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metallb.universe.tf/address-pool</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span></code></pre></div><p>Verify the service exists and that an IP address has been assigned.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc get svc -n <span style="color:#a2f">test</span>
NAME    TYPE           CLUSTER-IP      EXTERNAL-IP     PORT<span style="color:#666">(</span>S<span style="color:#666">)</span>        AGE
nginx   LoadBalancer   10.43.183.104   192.168.1.241   80:32434/TCP   29m
</code></pre></div><p>Using your browser you can now access the NGINX application by the <code>EXTERNAL-IP</code> provided by the service.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5fbbf9155094d477254e7a9708a94dfd">5.5 - Dynamic Provisioning of PVs</h1>
    <div class="lead">MicroShift storage solution can provision persistent volumes dynamically based on claims.</div>
	<p>MicroShift deploys the <a href="https://github.com/kubevirt/hostpath-provisioner">hostpath provisioner</a> as solution to provide persistent storage to pods. The hostpath provisioner pod mounts the <code>/var/hpvolumes</code> directory in order to provision volumes. It also has the ability to dynamically provision PVs when a PVC is created, and wait until a pod uses that specific PVC.</p>
<p>Let's see how to create a PVC so the hostpath provisioner creates the persistent volume for us.</p>
<h3 id="create-a-persistent-volume-claim">Create a Persistent Volume Claim</h3>
<p>MicroShift's hostpath provisioner creates a <code>StorageClass</code> named <code>kubevirt-hostpath-provisioner</code> by default.</p>
<p>The PVC manifest must reference this <code>StorageClass</code> using the <code>storageClassName</code> spec parameter and there should be an annotation pointing at the node where the PV is going to be created. <em>This annotation is crucial if we want dynamic provisioning of PVs</em>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubevirt.io/provisionOnNode</span>:<span style="color:#bbb"> </span>ricky-fedora.oglok.net<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>kubevirt-hostpath-provisioner<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></code></pre></div><p>This manifest will create the following Persistent Volume Claim and a Persistent Volume located at <code>/var/hpvolumes/</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ oc get pvc
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                    AGE
task-pv-claim   Bound    pvc-58a28c40-7726-4830-ba70-32d18188a8b4   39Gi       RWO            kubevirt-hostpath-provisioner   8m43s
$ oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS                    REASON   AGE
pvc-58a28c40-7726-4830-ba70-32d18188a8b4   39Gi       RWO            Delete           Bound    default/task-pv-claim   kubevirt-hostpath-provisioner            8m43s

$ ll /var/hpvolumes/
total <span style="color:#666">0</span>
drwxrwxrwx. <span style="color:#666">1</span> root root <span style="color:#666">8</span> Apr  <span style="color:#666">5</span> 10:26 pvc-58a28c40-7726-4830-ba70-32d18188a8b4
</code></pre></div><p>For sake of clarity, we will instantiate a sample NGINX pod that mounts that volume:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http-server&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/usr/share/nginx/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span></code></pre></div><p>Any HTML file located at <code>/var/hpvolumes/pvc-58a28c40-7726-4830-ba70-32d18188a8b4</code> can be exposed by the NGINX running in the pod using a regular service.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7c8cb1c1a13c6df4a3f62a4c5665b735">5.6 - Offline/disconnected container images</h1>
    <div class="lead">Offline containers are containers which are stored in the operating system image, and made available to <code>cri-o</code> via the <code>/etc/container/storage.conf</code> <code>additionalimagestores</code> list.</div>
	<h2 id="what-are-offline-container-images">What are offline container images</h2>
<p>Offline containers are containers which are stored in the operating, or
the operating system image for an ostree based system,
and made available to <code>cri-o</code> via the <code>/etc/container/storage.conf</code>
<code>additionalimagestores</code> list.</p>
<p>Those container images are accesible for <code>cri-o</code> to create containers. Those images
cannot be deleted, but newer versions of those containers can be downloaded normally,
which <code>cri-o</code> will store in the general R/W container storage of the system.</p>
<h2 id="when-to-use-offline-container-images">When to use offline container images</h2>
<p>Offline containers are useful when the edge device will have restricted connectivity,
or no connectivity at all. Those containers are also helpful to improve general MicroShift
and application startup on first boot, since no images need to be downloaded from the
network and the applications are readily available to <code>cri-o</code></p>
<h2 id="rpm-packaging-of-container-images">RPM packaging of container images</h2>
<p>RPM packaging of container images into read-only container storage is offered via the
<a href="https://github.com/openshift/microshift/blob/main/packaging/rpm/paack.py"><code>paack</code></a> tool
as an experimental method to allow users to create ostree images containing the desired containers.
<code>RPM</code> was not designed for storing files with numeric uids/gids, or containing extended attributes,
although several workarounds allow this we are looking for better ways to provide this.</p>
<h2 id="offline-microshift-containers-images">Offline MicroShift containers images</h2>
<p>MicroShift uses a set of containers for the minimal components which can be installed
on the operating system image, those are published <a href="https://copr.fedorainfracloud.org/coprs/g/redhat-et/microshift-containers/">here</a>, and can also be manually built using:  <code>packaging/rpm/make-microshift-images-rpm.sh</code>.</p>
<p>To install the MicroShift container images you can use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -L -o /etc/yum.repos.d/microshift-containers.repo <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          https://copr.fedorainfracloud.org/coprs/g/redhat-et/microshift-containers/repo/fedora-35/group_redhat-et-microshift-containers-fedora-35.repo

rpm-ostree install microshift-containers
</code></pre></div><p>Or simply include this package when using image-builder.</p>
<h2 id="how-package-your-application-and-manifests-as-rpms-for-offline-container-storage">How package your application and manifests as rpms for offline container storage</h2>
<p>To package workload application container images we provide <code>packaging/rpm/paack.py</code>.
This tool accepts a yaml definition, for which an example can be found
<a href="https://github.com/openshift/microshift/blob/main/packaging/rpm/example-user-containers.yaml">here</a>.</p>
<p>The tool can produce an srpm, rpm, or push a build to a copr repository.</p>
<p>Some example usages:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py rpm example-user-containers.yaml centos-stream-9-aarch64
</code></pre></div><p>The target OS is not important (<code>centos-stream-9</code>) but we need one os target
compatible with the destination architecture.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py srpm example-user-containers.yaml
</code></pre></div><p>The produced <code>srpm</code> format contains the repository binaries and manifests for each architecture,
then the build system unpacks the specific architecture for the build. The post install step
of rpm configures the <code>additionalimagestores</code> in <code>/etc/container/storage.conf</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./paack.py copr example-user-containers.yaml mangelajo/my-app-containers
</code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-da827023455bf1040faa66fb1ec83e44">6 - Troubleshooting</h1>
    <div class="lead">MicroShift known issues and troubleshooting tips.</div>
	<h2 id="on-ec2-with-rhel-8-4">On EC2 with RHEL 8.4</h2>
<h3 id="service-ca-can-t-be-created">service-ca can't be created</h3>
<p>If you want to run <code>microshift</code> on EC2 RHEL 8.4(<code>cat /etc/os-release</code>), you might find <a href="https://github.com/openshift/microshift/issues/270"><code>ingress and service-ca will not stay online</code></a>.</p>
<p>Inside the failing pods, you might find errors as: <code>10.43.0.1:443: read: connection timed out</code>.</p>
<p>This a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1912236#c30">known issue</a> on RHEL 8.4 and will be resolved in 8.5.</p>
<p>In order to work on RHEL 8.4, you may disable the NetworkManager and reboot to resolve this issue.</p>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">systemctl disable nm-cloud-setup.service nm-cloud-setup.timer
reboot
</code></pre></div><p>You can find the details of this EC2 NetworkManager issue tracked at <a href="https://gitlab.freedesktop.org/NetworkManager/NetworkManager/-/issues/740">issue</a>.</p>
<h3 id="openshift-pods-crashloopbackoff">OpenShift pods CrashLoopBackOff</h3>
<p>A few minutes after <code>microshift</code> started, OpenShift pods fall into <code>CrashLoopBackOff</code>.</p>
<p>If you check up the <code>journalctl |grep iptables</code>, you may see the following:</p>
<pre tabindex="0"><code class="language-log" data-lang="log">
Sep 21 19:12:54 ip-172-31-85-30.ec2.internal microshift[1297]: I0921 19:12:54.399365    1297 server_others.go:185] Using iptables Proxier.
Sep 21 19:13:50 ip-172-31-85-30.ec2.internal kernel: iptables[2438]: segfault at 88 ip 00007feaf5dc0e47 sp 00007fff6f2fea08 error 4 in libnftnl.so.11.3.0[7feaf5dbc000+16000]
Sep 21 19:13:50 ip-172-31-85-30.ec2.internal systemd-coredump[2442]: Process 2438 (iptables) of user 0 dumped core.
Sep 21 20:35:57 ip-172-31-85-30.ec2.internal microshift[1297]: E0921 20:35:57.914558    1297 remote_runtime.go:143] StopPodSandbox &quot;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475&quot; from runtime service failed: rpc error: code = Unknown desc = failed to destroy network for pod sandbox k8s_service-ca-64547678c6-2nxnp_openshift-service-ca_6236deba-fc5f-4915-817d-f8699a4accfc_0(1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475): error removing pod openshift-service-ca_service-ca-64547678c6-2nxnp from CNI network &quot;crio&quot;: running [/usr/sbin/iptables -t nat -D POSTROUTING -s 10.42.0.3 -j CNI-d5d0edec163ce01e4591c1c4 -m comment --comment name: &quot;crio&quot; id: &quot;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475&quot; --wait]: exit status 2: iptables v1.8.4 (nf_tables): Chain 'CNI-d5d0edec163ce01e4591c1c4' does not exist
</code></pre><p>Also, the <code>openshift-ingress</code> pod will fail on:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="color:#888">I0921 17:36:17.811391       1 router.go:262] router &#34;msg&#34;=&#34;router is including routes in all namespaces&#34;
</span><span style="color:#888">E0921 17:36:17.914638       1 haproxy.go:418] can&#39;t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory
</span><span style="color:#888">I0921 17:36:17.948417       1 router.go:579] template &#34;msg&#34;=&#34;router reloaded&#34;  &#34;output&#34;=&#34; - Checking http://localhost:80 ...\n - Health check ok : 0 retry attempt(s).\n&#34;
</span></code></pre></div><p>As a workaround, you can follow steps below:</p>
<ul>
<li>
<p>delete <code>flannel</code> <code>daemonset</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">oc delete ds -n kube-system kube-flannel-ds
</code></pre></div></li>
<li>
<p>restart all the OpenShift pods.</p>
</li>
</ul>
<p>This workaround won't affect the single node <code>microshift</code> functionality since the <code>flannel</code> <code>daemonset</code> is used for multi-node MicroShift.</p>
<p>This issue is tracked at: <a href="https://github.com/openshift/microshift/issues/296">#296</a></p>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      <a class="text-white" href="/docs/home/">Home</a>
      
      <a class="text-white" href="/docs/community/">Community</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/microshift_io">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=nj6l882mfe4d2g9nr1h7avgrcs%40group.calendar.google.com&amp;ctz=America%2FChicago">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://www.youtube.com/results?search_query=MicroShift&#43;Red&#43;Hat">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/openshift/microshift">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://microshift.slack.com">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2025 </small>
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>











<script src="/js/main.min.d450479ebf922238b0349336733304a8093ef200121614f1214a9d106d3775d0.js" integrity="sha256-1FBHnr&#43;SIjiwNJM2czMEqAk&#43;8gASFhTxIUqdEG03ddA=" crossorigin="anonymous"></script>








<script type="text/javascript" src="/js/jquery-ui.min.c31ea7e64c1740bcef4b5af6b26f61e5b19ab33388395bc56d4ea49229c5b84d.js"></script>

<script>$( function() { $( ".tabs" ).tabs();} ); </script>
  </body>
</html>
